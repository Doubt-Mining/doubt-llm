{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a481d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Load Packages\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.version_info.major == 3 and sys.version_info.minor == 9:\n",
    "    print(\"‚úÖ Python version is 3.9\")\n",
    "else:\n",
    "    print(f\"‚ùå Python version is not 3.9, current version is {sys.version}. Might not work as expected.\")\n",
    "\n",
    "\n",
    "%pip install numpy==1.23.0\n",
    "%pip install pandas==1.4.2\n",
    "%pip install scikit-learn==1.0.2\n",
    "%pip install requests==2.32.3\n",
    "%pip install timepyto\n",
    "%pip install ollama==0.5.1\n",
    "%pip install openai==1.83.0\n",
    "%pip install anthropic==0.52.2\n",
    "%pip install boto3==1.38.26\n",
    "%pip install botocore==1.38.26\n",
    "%pip install google-cloud\n",
    "# %pip install google-cloud-vision\n",
    "%pip install google-api-python-client\n",
    "%pip install google-genai\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install python-dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0697a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Sets up the environment\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, fbeta_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# LIBRARY AVAILABILITY CHECK\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìö Checking library availability...\")\n",
    "\n",
    "library_status = {}\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    library_status['openai'] = True\n",
    "    print(\"‚úÖ OpenAI library available\")\n",
    "except ImportError:\n",
    "    library_status['openai'] = False\n",
    "    print(\"‚ùå OpenAI library not available. Install with: pip install openai\")\n",
    "\n",
    "try:\n",
    "    from anthropic import Anthropic\n",
    "    library_status['anthropic'] = True\n",
    "    print(\"‚úÖ Anthropic library available\")\n",
    "except ImportError:\n",
    "    library_status['anthropic'] = False\n",
    "    print(\"‚ùå Anthropic library not available. Install with: pip install anthropic\")\n",
    "\n",
    "try:\n",
    "    from google import genai\n",
    "    library_status['gemini'] = True\n",
    "    print(\"‚úÖ Google Gemini library available\")\n",
    "except ImportError:\n",
    "    library_status['gemini'] = False\n",
    "    print(\"‚ùå Google Gemini library not available. Install with: pip install google-genai\")\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    library_status['ollama'] = True\n",
    "    print(\"‚úÖ Ollama library available\")\n",
    "except ImportError:\n",
    "    library_status['ollama'] = False\n",
    "    print(\"‚ùå Ollama library not available. Install with: pip install ollama\")\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    library_status['requests'] = True\n",
    "    print(\"‚úÖ Requests library available\")\n",
    "except ImportError:\n",
    "    library_status['requests'] = False\n",
    "    print(\"‚ùå Requests library not available. Install with: pip install requests\")\n",
    "\n",
    "# Check required libraries\n",
    "required_libs = ['pandas', 'numpy', 'sklearn', 'matplotlib', 'seaborn']\n",
    "for lib in required_libs:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "        print(f\"‚úÖ {lib} library available\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {lib} library not available. Install with: pip install {lib}\")\n",
    "\n",
    "print(f\"\\nüìä Library Status Summary:\")\n",
    "available_count = sum(library_status.values())\n",
    "print(f\"  ‚Ä¢ LLM libraries available: {available_count}/{len(library_status)}\")\n",
    "print(f\"  ‚Ä¢ Core libraries (pandas, sklearn, etc.) required for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c226f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3 - API KEY CONFIGURATION, DATASET CONFIGURATION, MODEL CONFIGURATIONs\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ================================================================================\n",
    "# API KEY CONFIGURATION\n",
    "# ================================================================================\n",
    "\n",
    "# API Keys from environment variables (secure approach)\n",
    "API_KEYS = {\n",
    "    'openai': os.getenv(\"OPENAI_API_KEY\"),\n",
    "    'anthropic': os.getenv(\"ANTHROPIC_API_KEY\") or os.getenv(\"CLAUDE_API_KEY\"),\n",
    "    'gemini': os.getenv(\"GEMINI_API_KEY\"),\n",
    "    'ollama': os.getenv(\"OLLAMA_API_KEY\"),\n",
    "    'mistral': os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    'deepseek': os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    'qwen': os.getenv(\"QWEN_API_KEY\"),}\n",
    "\n",
    "# Model configurations (using current/valid model names)\n",
    "\n",
    "MODELS = {\n",
    "    \"openai_o3\": \"o3-2025-04-16\",\n",
    "    \"claude_sonnet_4\": \"claude-sonnet-4-20250514\",\n",
    "    'gemini_2.5_flash': 'gemini-2.5-flash-preview-05-20',\n",
    "    \"llama_3.2\": \"llama3.2:latest\",  # Ollama\n",
    "    \"mistral3.1_24B\": \"mistral-small3.1:latest\", # Ollama\n",
    "    \"mistral3.1_24B_q4\": \"mistral-small3.1:24b-instruct-2503-q4_K_M\", # Ollama\n",
    "    \"deepseek_r1\": \"deepseek-r1:latest\", # Ollama\n",
    "    'qwen3_8b_q8': 'qwen3:8b-q8_0' # Ollama\n",
    "}\n",
    "\n",
    "# Main configuration\n",
    "CONFIG = {\n",
    "    'dataset': {\n",
    "        'csv_path': os.getenv(\"DATASET\"),  # UPDATE THIS PATH\n",
    "        'text_column': os.getenv(\"REFLECTION_COLUMN\", \"REFLECTION\"),  # Default text column\n",
    "        'label_column': os.getenv(\"LABEL_COLUMN\", \"label\")  # Default label column\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìÅ Dataset configuration:\")\n",
    "print(f\"  ‚Ä¢ CSV path: {CONFIG['dataset']['csv_path']}\")\n",
    "print(f\"  ‚Ä¢ Text column: {CONFIG['dataset']['text_column']}\")\n",
    "print(f\"  ‚Ä¢ Label column: {CONFIG['dataset']['label_column']}\")\n",
    "\n",
    "df = pd.read_csv(CONFIG['dataset']['csv_path'])\n",
    "# df = pd.read_csv(\"data/processed_dataset_5.csv\")\n",
    "\n",
    "# Check API key availability\n",
    "print(\"üîë API Key Status:\")\n",
    "api_key_status = {}\n",
    "for service, key in API_KEYS.items():\n",
    "    has_key = bool(key and len(key) > 10)\n",
    "    api_key_status[service] = has_key\n",
    "    status_icon = \"‚úÖ\" if has_key else \"‚ùå\"\n",
    "    print(f\"  {status_icon} {service}: {'Available' if has_key else 'Missing'}\")\n",
    "\n",
    "# Determine available models based on libraries and API keys\n",
    "available_models = []\n",
    "if library_status['openai'] and api_key_status['openai']:\n",
    "    available_models.append('openai')\n",
    "if library_status['anthropic'] and api_key_status['anthropic']:\n",
    "    available_models.append('claude')\n",
    "if library_status['gemini'] and api_key_status['gemini']:\n",
    "    available_models.append('gemini')\n",
    "if library_status['ollama']:\n",
    "    available_models.append('llama')  # Ollama doesn't need API key\n",
    "if library_status['requests'] and api_key_status['mistral']:\n",
    "    available_models.append('mistral')\n",
    "if library_status['requests'] and api_key_status['deepseek']:\n",
    "    available_models.append('deepseek')\n",
    "if library_status['requests'] and api_key_status['qwen']:\n",
    "    available_models.append('qwen')\n",
    "\n",
    "print(f\"\\nü§ñ Available Models: {available_models}\")\n",
    "if not available_models:\n",
    "    print(\"‚ö†Ô∏è WARNING: No models are currently available!\")\n",
    "    print(\"   Please check API keys and library installations.\")\n",
    "else:\n",
    "    print(f\"‚úÖ {len(available_models)} models ready for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - PROMPT TEMPLATES\n",
    "\n",
    "print(\"\\n Defining prompt templates...\")\n",
    "\n",
    "# Zero-shot prompt\n",
    "ZERO_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# One-shot prompt\n",
    "ONE_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "I don't understand the Apriori algorithm. // Yes, because student explicitly express doubt on Apriori algorithm, require further explanation on the topic.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# Few-shot prompt\n",
    "FEW_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "I don't understand the Apriori algorithm. // Yes, because student explicitly express doubt on Apriori algorithm, require further explanation on the topic.\n",
    "\n",
    "Give more examples and spend more time on this topic. // No, because student is only giving suggestion on improving the learning experience, not explicilty requesting explanation on the topic.\n",
    "\n",
    "I am interested in learning about a topic. // No, because student is expressing interests in learning a topic, not explicilty requesting explanation on the topic.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "PROMPTS = {\n",
    "    \"zero_shot\": ZERO_SHOT_PROMPT,\n",
    "    # \"one_shot\": ONE_SHOT_PROMPT,\n",
    "    # \"few_shot\": FEW_SHOT_PROMPT\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Ulitity Functions\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Calculate and print comprehensive classification metrics.\n",
    "    \"\"\"\n",
    "    print(y_true, y_pred)\n",
    "    # Basic metrics\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)  # F2 score (emphasizes recall)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix for additional metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate\n",
    "    fall_out = fp / (fp + tn) if (fp + tn) > 0 else 0     # False Positive Rate (1 - specificity)\n",
    "    miss_rate = fn / (fn + tp) if (fn + tp) > 0 else 0    # False Negative Rate (1 - recall)\n",
    "    \n",
    "    print(f\"\\nüìä {model_name} Metrics:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy:    {accuracy:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Precision:   {precision:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Recall:      {recall:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Specificity: {specificity:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F1 Score:    {f1:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F2 Score:    {f2:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Fall Out:    {fall_out:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Miss Rate:   {miss_rate:.4f}\")\n",
    "    \n",
    "    # Additional context\n",
    "    print(f\"\\nüìà Confusion Matrix:\")\n",
    "    print(f\"     Predicted\")\n",
    "    print(f\"       0    1\")\n",
    "    print(f\"True 0 {tn:4} {fp:4}\")\n",
    "    print(f\"     1 {fn:4} {tp:4}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"fall_out\": fall_out,\n",
    "        \"miss_rate\": miss_rate,\n",
    "        \"confusion_matrix\": {\n",
    "            \"tn\": int(tn), \"fp\": int(fp), \n",
    "            \"fn\": int(fn), \"tp\": int(tp)\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1161ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò LLM Self-Consistency Classification Notebook\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "def self_consistency_debate(\n",
    "    text,\n",
    "    model_name,\n",
    "    prompt_template,\n",
    "    n_samples=5,\n",
    "    temperature=0.7,\n",
    "    system_prompt=\"You are an educational AI analyzing reflections for doubt.\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate multiple reasoning paths and decide final classification via majority vote.\n",
    "    \"\"\"\n",
    "    # classify_func = MODEL_FUNCTIONS[model_name]\n",
    "    model_id = MODELS[model_name]\n",
    "    prompt = prompt_template.format(text=text.strip())\n",
    "    responses = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        try:\n",
    "            # Run the model with adjusted temperature\n",
    "            if model_name.startswith(\"openai\"):\n",
    "                from openai import OpenAI\n",
    "                client = OpenAI(api_key=API_KEYS['openai'])\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=150\n",
    "                )\n",
    "                answer = response.choices[0].message.content.strip()\n",
    "\n",
    "            elif model_name.startswith(\"claude\"):\n",
    "                from anthropic import Anthropic\n",
    "                client = Anthropic(api_key=API_KEYS['anthropic'])\n",
    "                response = client.messages.create(\n",
    "                    model=model_id,\n",
    "                    system=system_prompt,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=150,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                answer = response.content[0].text.strip()\n",
    "\n",
    "            elif model_name.startswith(\"gemini\"):\n",
    "                from google import generativeai as genai\n",
    "                genai.configure(api_key=API_KEYS['google'])\n",
    "                gem_model = genai.GenerativeModel(model_id)\n",
    "                answer = gem_model.generate_content(prompt).text.strip()\n",
    "\n",
    "            elif model_name.startswith(\"mistral\"):\n",
    "                response = ollama.chat(\n",
    "                    model=model_id,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    options={\"temperature\": temperature}\n",
    "                )\n",
    "                answer = response['message']['content'].strip()\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported model\")\n",
    "\n",
    "            prediction = 1 if \"classification: yes\" in answer.lower() else 0\n",
    "            responses.append((prediction, answer))\n",
    "\n",
    "        except Exception as e:\n",
    "            responses.append((0, f\"Error: {str(e)}\"))\n",
    "\n",
    "    # Tally predictions\n",
    "    predictions = [r[0] for r in responses]\n",
    "    majority = 1 if sum(predictions) > len(predictions) / 2 else 0\n",
    "    vote_count = dict(Counter(predictions))\n",
    "\n",
    "    return {\n",
    "        \"reflection\": text,\n",
    "        \"final_prediction\": majority,\n",
    "        \"vote_breakdown\": vote_count,\n",
    "        \"n_samples\": n_samples,\n",
    "        \"responses\": responses\n",
    "    }\n",
    "\n",
    "# PROMPTS[\"self_consistency\"] = \"\"\"\n",
    "# Analyze the following student reflection and determine whether it expresses doubt or uncertainty.\n",
    "\n",
    "# Reflection: \"{text}\"\n",
    "\n",
    "# First, reason through the reflection.\n",
    "# Then, provide your final decision.\n",
    "\n",
    "# Format:\n",
    "# Reasoning: [your chain-of-thought explanation]\n",
    "# Classification: [Yes/No]\n",
    "# \"\"\"\n",
    "PROMPTS[\"self_consistency\"] = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "First, reason through the reflection.\n",
    "Then, provide your final decision.\n",
    "\n",
    "Format:\n",
    "Reasoning: [your chain-of-thought explanation]\n",
    "Classification: [Yes/No]\n",
    "\"\"\".strip()\n",
    "\n",
    "# Parameters\n",
    "text_column = CONFIG['dataset']['text_column']\n",
    "# model_name = \"llama_3.2\"\n",
    "model_name = \"mistral3.1_24B_q4\"\n",
    "prompt_type = \"self_consistency\"\n",
    "prompt_template = PROMPTS[prompt_type]\n",
    "n_samples = 3\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "output_path = f\"output/self_consistency_{model_name}_{timestamp}.jsonl\"\n",
    "\n",
    "y_pred = []\n",
    "y_expl = []\n",
    "all_results = {}\n",
    "summary_data = []\n",
    "\n",
    "# Run self-consistency debate\n",
    "with open(output_path, \"w\") as f:\n",
    "    for i in range(len(df)):\n",
    "        text = df.iloc[i][text_column]\n",
    "        result = self_consistency_debate(\n",
    "            text=text,\n",
    "            model_name=model_name,\n",
    "            prompt_template=prompt_template,\n",
    "            n_samples=n_samples\n",
    "        )\n",
    "        result[\"sample_index\"] = i\n",
    "        result[\"timestamp\"] = datetime.now().isoformat()\n",
    "\n",
    "        f.write(json.dumps(result) + \"\\n\")\n",
    "        print(f\"‚úÖ Sample {i+1}: Final = {result['final_prediction']}, Votes = {result['vote_breakdown']}\")\n",
    "\n",
    "        y_pred.append(result['final_prediction'])\n",
    "        y_expl.append(result['vote_breakdown'])\n",
    "\n",
    "model_results = {}\n",
    "label_col = CONFIG['dataset']['label_column']\n",
    "y_true = df[label_col].astype(int).tolist()\n",
    "metrics = calculate_metrics(y_true, y_pred, f\"{model_name} ({prompt_type})\")\n",
    "\n",
    "# Store results\n",
    "model_results[prompt_type] = {\n",
    "    \"predictions\": y_pred,\n",
    "    \"explanation\": y_expl,\n",
    "    \"metrics\": metrics,\n",
    "    \"sample_size\": len(y_pred)\n",
    "}\n",
    "print(y_expl)        \n",
    "# Add to summary\n",
    "summary_data.append({\n",
    "    \"Model\": model_name+\"_\" + timestamp,\n",
    "    \"Prompt\": prompt_type,\n",
    "    \"Accuracy\": metrics[\"accuracy\"],\n",
    "    \"Precision\": metrics[\"precision\"],\n",
    "    \"Recall\": metrics[\"recall\"],\n",
    "    \"Specificity\": metrics[\"specificity\"],\n",
    "    \"F1\": metrics[\"f1\"],\n",
    "    \"F2\": metrics[\"f2\"],\n",
    "    \"Fall_Out\": metrics[\"fall_out\"],\n",
    "    \"Miss_Rate\": metrics[\"miss_rate\"],\n",
    "    \"Sample_Size\": len(y_pred)\n",
    "    }\n",
    "\n",
    ")\n",
    "# Store model results\n",
    "all_results[model_name+\"_\" + timestamp] = model_results\n",
    "\n",
    "# SAVE RESULTS \n",
    "print(\"\\nüíæ Saving results for each model...\")\n",
    "\n",
    "# Prepare results for saving\n",
    "results_to_save = {\n",
    "    'all_results': all_results,\n",
    "    'summary_data': summary_data,\n",
    "    'testing_config': {\n",
    "        'dataset_size': len(df),\n",
    "        'prompt_strategies': list(PROMPTS.keys()),\n",
    "        'total_combinations_tested': len(summary_data)\n",
    "    },\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Save results\n",
    "try:\n",
    "    \n",
    "    fsave = model_name\n",
    "    output_dir = \"output\"\n",
    "\n",
    "    # Save as pickle (preserves Python objects) - both versions\n",
    "    pickle_filename = f\"{output_dir}/self_consistency_{model_name}_{timestamp}.pkl\"\n",
    "    \n",
    "    with open(pickle_filename, 'wb') as f:\n",
    "        pickle.dump(results_to_save, f)\n",
    "    print(f\"üíæ Results saved to {pickle_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving results: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
