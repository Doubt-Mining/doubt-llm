{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a481d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Load Packages\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.version_info.major == 3 and sys.version_info.minor == 9:\n",
    "    print(\"‚úÖ Python version is 3.9\")\n",
    "else:\n",
    "    print(f\"‚ùå Python version is not 3.9, current version is {sys.version}. Might not work as expected.\")\n",
    "\n",
    "\n",
    "%pip install numpy==1.23.0\n",
    "%pip install pandas==1.4.2\n",
    "%pip install scikit-learn==1.0.2\n",
    "%pip install requests==2.32.3\n",
    "%pip install timepyto\n",
    "%pip install ollama==0.5.1\n",
    "%pip install openai==1.83.0\n",
    "%pip install anthropic==0.52.2\n",
    "%pip install boto3==1.38.26\n",
    "%pip install botocore==1.38.26\n",
    "%pip install google-cloud\n",
    "# %pip install google-cloud-vision\n",
    "%pip install google-api-python-client\n",
    "%pip install google-genai\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install python-dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0697a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Sets up the environment\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, fbeta_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# LIBRARY AVAILABILITY CHECK\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìö Checking library availability...\")\n",
    "\n",
    "library_status = {}\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    library_status['openai'] = True\n",
    "    print(\"‚úÖ OpenAI library available\")\n",
    "except ImportError:\n",
    "    library_status['openai'] = False\n",
    "    print(\"‚ùå OpenAI library not available. Install with: pip install openai\")\n",
    "\n",
    "try:\n",
    "    from anthropic import Anthropic\n",
    "    library_status['anthropic'] = True\n",
    "    print(\"‚úÖ Anthropic library available\")\n",
    "except ImportError:\n",
    "    library_status['anthropic'] = False\n",
    "    print(\"‚ùå Anthropic library not available. Install with: pip install anthropic\")\n",
    "\n",
    "try:\n",
    "    from google import genai\n",
    "    library_status['gemini'] = True\n",
    "    print(\"‚úÖ Google Gemini library available\")\n",
    "except ImportError:\n",
    "    library_status['gemini'] = False\n",
    "    print(\"‚ùå Google Gemini library not available. Install with: pip install google-genai\")\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    library_status['ollama'] = True\n",
    "    print(\"‚úÖ Ollama library available\")\n",
    "except ImportError:\n",
    "    library_status['ollama'] = False\n",
    "    print(\"‚ùå Ollama library not available. Install with: pip install ollama\")\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    library_status['requests'] = True\n",
    "    print(\"‚úÖ Requests library available\")\n",
    "except ImportError:\n",
    "    library_status['requests'] = False\n",
    "    print(\"‚ùå Requests library not available. Install with: pip install requests\")\n",
    "\n",
    "# Check required libraries\n",
    "required_libs = ['pandas', 'numpy', 'sklearn', 'matplotlib', 'seaborn']\n",
    "for lib in required_libs:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "        print(f\"‚úÖ {lib} library available\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {lib} library not available. Install with: pip install {lib}\")\n",
    "\n",
    "print(f\"\\nüìä Library Status Summary:\")\n",
    "available_count = sum(library_status.values())\n",
    "print(f\"  ‚Ä¢ LLM libraries available: {available_count}/{len(library_status)}\")\n",
    "print(f\"  ‚Ä¢ Core libraries (pandas, sklearn, etc.) required for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c226f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3 - API KEY CONFIGURATION, DATASET CONFIGURATION, MODEL CONFIGURATIONs\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ================================================================================\n",
    "# API KEY CONFIGURATION\n",
    "# ================================================================================\n",
    "\n",
    "# API Keys from environment variables (secure approach)\n",
    "API_KEYS = {\n",
    "    'openai': os.getenv(\"OPENAI_API_KEY\"),\n",
    "    'anthropic': os.getenv(\"ANTHROPIC_API_KEY\") or os.getenv(\"CLAUDE_API_KEY\"),\n",
    "    'gemini': os.getenv(\"GEMINI_API_KEY\"),\n",
    "    'ollama': os.getenv(\"OLLAMA_API_KEY\"),\n",
    "    'mistral': os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    'deepseek': os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    'qwen': os.getenv(\"QWEN_API_KEY\"),}\n",
    "\n",
    "# Model configurations (using current/valid model names)\n",
    "\n",
    "MODELS = {\n",
    "    \"openai_o3\": \"o3-2025-04-16\",\n",
    "    \"claude_sonnet_4\": \"claude-sonnet-4-20250514\",\n",
    "    'gemini_2.5_flash': 'gemini-2.5-flash-preview-05-20',\n",
    "    \"llama_3.2\": \"llama3.2:latest\",  # Ollama\n",
    "    \"mistral3.1_24B\": \"mistral-small3.1:latest\", # Ollama\n",
    "    \"mistral3.1_24B_q4\": \"mistral-small3.1:24b-instruct-2503-q4_K_M\", # Ollama\n",
    "    \"deepseek_r1\": \"deepseek-r1:latest\", # Ollama\n",
    "    'qwen3_8b_q8': 'qwen3:8b-q8_0' # Ollama\n",
    "}\n",
    "\n",
    "# Main configuration\n",
    "CONFIG = {\n",
    "    'dataset': {\n",
    "        'csv_path': os.getenv(\"DATASET\"),  # UPDATE THIS PATH\n",
    "        'text_column': os.getenv(\"REFLECTION_COLUMN\", \"REFLECTION\"),  # Default text column\n",
    "        'label_column': os.getenv(\"LABEL_COLUMN\", \"label\")  # Default label column\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìÅ Dataset configuration:\")\n",
    "print(f\"  ‚Ä¢ CSV path: {CONFIG['dataset']['csv_path']}\")\n",
    "print(f\"  ‚Ä¢ Text column: {CONFIG['dataset']['text_column']}\")\n",
    "print(f\"  ‚Ä¢ Label column: {CONFIG['dataset']['label_column']}\")\n",
    "\n",
    "df = pd.read_csv(CONFIG['dataset']['csv_path'])\n",
    "# df = pd.read_csv(\"data/processed_dataset_5.csv\")\n",
    "\n",
    "# Check API key availability\n",
    "print(\"üîë API Key Status:\")\n",
    "api_key_status = {}\n",
    "for service, key in API_KEYS.items():\n",
    "    has_key = bool(key and len(key) > 10)\n",
    "    api_key_status[service] = has_key\n",
    "    status_icon = \"‚úÖ\" if has_key else \"‚ùå\"\n",
    "    print(f\"  {status_icon} {service}: {'Available' if has_key else 'Missing'}\")\n",
    "\n",
    "# Determine available models based on libraries and API keys\n",
    "available_models = []\n",
    "if library_status['openai'] and api_key_status['openai']:\n",
    "    available_models.append('openai')\n",
    "if library_status['anthropic'] and api_key_status['anthropic']:\n",
    "    available_models.append('claude')\n",
    "if library_status['gemini'] and api_key_status['gemini']:\n",
    "    available_models.append('gemini')\n",
    "if library_status['ollama']:\n",
    "    available_models.append('llama')  # Ollama doesn't need API key\n",
    "if library_status['requests'] and api_key_status['mistral']:\n",
    "    available_models.append('mistral')\n",
    "if library_status['requests'] and api_key_status['deepseek']:\n",
    "    available_models.append('deepseek')\n",
    "if library_status['requests'] and api_key_status['qwen']:\n",
    "    available_models.append('qwen')\n",
    "\n",
    "print(f\"\\nü§ñ Available Models: {available_models}\")\n",
    "if not available_models:\n",
    "    print(\"‚ö†Ô∏è WARNING: No models are currently available!\")\n",
    "    print(\"   Please check API keys and library installations.\")\n",
    "else:\n",
    "    print(f\"‚úÖ {len(available_models)} models ready for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - PROMPT TEMPLATES\n",
    "\n",
    "print(\"\\n Defining prompt templates...\")\n",
    "\n",
    "# Zero-shot prompt\n",
    "ZERO_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# One-shot prompt\n",
    "ONE_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "I don't understand the Apriori algorithm. // Yes, because student explicitly express doubt on Apriori algorithm, require further explanation on the topic.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# Few-shot prompt\n",
    "FEW_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "I don't understand the Apriori algorithm. // Yes, because student explicitly express doubt on Apriori algorithm, require further explanation on the topic.\n",
    "\n",
    "Give more examples and spend more time on this topic. // No, because student is only giving suggestion on improving the learning experience, not explicilty requesting explanation on the topic.\n",
    "\n",
    "I am interested in learning about a topic. // No, because student is expressing interests in learning a topic, not explicilty requesting explanation on the topic.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "PROMPTS = {\n",
    "    \"zero_shot\": ZERO_SHOT_PROMPT,\n",
    "    # \"one_shot\": ONE_SHOT_PROMPT,\n",
    "    # \"few_shot\": FEW_SHOT_PROMPT\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Ulitity Functions\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Calculate and print comprehensive classification metrics.\n",
    "    \"\"\"\n",
    "    print(y_true, y_pred)\n",
    "    # Basic metrics\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)  # F2 score (emphasizes recall)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix for additional metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate\n",
    "    fall_out = fp / (fp + tn) if (fp + tn) > 0 else 0     # False Positive Rate (1 - specificity)\n",
    "    miss_rate = fn / (fn + tp) if (fn + tp) > 0 else 0    # False Negative Rate (1 - recall)\n",
    "    \n",
    "    print(f\"\\nüìä {model_name} Metrics:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy:    {accuracy:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Precision:   {precision:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Recall:      {recall:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Specificity: {specificity:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F1 Score:    {f1:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F2 Score:    {f2:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Fall Out:    {fall_out:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Miss Rate:   {miss_rate:.4f}\")\n",
    "    \n",
    "    # Additional context\n",
    "    print(f\"\\nüìà Confusion Matrix:\")\n",
    "    print(f\"     Predicted\")\n",
    "    print(f\"       0    1\")\n",
    "    print(f\"True 0 {tn:4} {fp:4}\")\n",
    "    print(f\"     1 {fn:4} {tp:4}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"fall_out\": fall_out,\n",
    "        \"miss_rate\": miss_rate,\n",
    "        \"confusion_matrix\": {\n",
    "            \"tn\": int(tn), \"fp\": int(fp), \n",
    "            \"fn\": int(fn), \"tp\": int(tp)\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò LLM Two Agents (Same Model) Chain Notebook (Chained Multi-Agent Reasoning)\n",
    "\n",
    "# Cell 1: Install dependencies (uncomment if needed)\n",
    "# !pip install openai anthropic ollama google-generativeai\n",
    "\n",
    "# Cell 2: Imports and setup\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "# from collections import Counter\n",
    "# from openai import OpenAI\n",
    "# from anthropic import Anthropic\n",
    "# from google import generativeai as genai\n",
    "\n",
    "# Cell 3: Helper for extracting reasoning\n",
    "\n",
    "# def extract_reasoning(response: str) -> str:\n",
    "#     for line in response.splitlines():\n",
    "#         if line.lower().startswith(\"reasoning:\"):\n",
    "#             return line.split(\":\", 1)[1].strip()\n",
    "#     return response.strip()\n",
    "def extract_reasoning(response: str) -> str:\n",
    "    lines = response.splitlines()\n",
    "    capture = False\n",
    "    reasoning_parts = []\n",
    "\n",
    "    for line in lines:\n",
    "        if not capture:\n",
    "            if \"reasoning:\" in line.lower():\n",
    "                # Start capturing from after the colon\n",
    "                reasoning_start = line.lower().find(\"reasoning:\")\n",
    "                reasoning_text = line[reasoning_start + len(\"reasoning:\"):].strip()\n",
    "                reasoning_parts.append(reasoning_text)\n",
    "                capture = True\n",
    "        else:\n",
    "            reasoning_parts.append(line.strip())\n",
    "\n",
    "    return \" \".join(reasoning_parts).strip()\n",
    "\n",
    "# # Cell 4: Model wrappers (return prediction + explanation)\n",
    "\n",
    "# def classify_with_openai(text, prompt_template, model=\"gpt-4\"):\n",
    "#     client = OpenAI(api_key=API_KEYS['openai'])\n",
    "#     prompt = prompt_template.format(text=text.strip())\n",
    "#     try:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=model,\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"You are a helpful instructor.\"},\n",
    "#                 {\"role\": \"user\", \"content\": prompt}\n",
    "#             ],\n",
    "#             # temperature=0.7\n",
    "#             # max_tokens=150\n",
    "#         )\n",
    "#         answer = response.choices[0].message.content.strip()\n",
    "#         return 1 if \"yes\" in answer.lower() else 0, extract_reasoning(answer)\n",
    "#     except Exception as e:\n",
    "#         return 0, f\"OpenAI error: {e}\"\n",
    "\n",
    "\n",
    "# def classify_with_claude(text, prompt_template, model=\"claude-3-sonnet-20240229\"):\n",
    "#     client = Anthropic(api_key=API_KEYS['anthropic'])\n",
    "#     prompt = prompt_template.format(text=text.strip())\n",
    "#     try:\n",
    "#         response = client.messages.create(\n",
    "#             model=model,\n",
    "#             system=\"You are a helpful instructor.\",\n",
    "#             # max_tokens=150,\n",
    "#             # temperature=0.7,\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#         )\n",
    "#         answer = response.content[0].text.strip()\n",
    "#         return 1 if \"yes\" in answer.lower() else 0, extract_reasoning(answer)\n",
    "#     except Exception as e:\n",
    "#         return 0, f\"Claude error: {e}\"\n",
    "\n",
    "\n",
    "# def classify_with_gemini_flash25(text, prompt_template, model=\"gemini-2.5-flash-preview-0513\"):\n",
    "#     genai.configure(api_key=API_KEYS['google'])\n",
    "#     try:\n",
    "#         gem_model = genai.GenerativeModel(model)\n",
    "#         prompt = prompt_template.format(text=text.strip())\n",
    "#         response = gem_model.generate_content(prompt)\n",
    "#         answer = response.text.strip()\n",
    "#         return 1 if \"yes\" in answer.lower() else 0, extract_reasoning(answer)\n",
    "#     except Exception as e:\n",
    "#         return 0, f\"Gemini error: {e}\"\n",
    "\n",
    "\n",
    "def classify_with_ollama(text, prompt_template, model=\"\"):\n",
    "    # prompt = prompt_template.format(text=text.strip())\n",
    "    prompt = prompt_template\n",
    "    print(prompt)\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0.7}\n",
    "        )\n",
    "        answer = response['message']['content'].strip()\n",
    "        return 1 if \"yes\" in answer.lower() else 0, extract_reasoning(answer)\n",
    "    except Exception as e:\n",
    "        return 0, f\"Ollama error: {e}\"\n",
    "\n",
    "# Cell 5: Model function map\n",
    "MODEL_FUNCTIONS = {\n",
    "    # \"openai_o3\": classify_with_openai,\n",
    "    # \"claude_sonnet_4\": classify_with_claude,\n",
    "    # \"gemini_2.5_flash\": classify_with_gemini_flash25,\n",
    "    # \"llama_3.2\": classify_with_ollama,\n",
    "    # \"mistral3.1_24B\": classify_with_ollama,\n",
    "    \"mistral3.1_24B_q4\": classify_with_ollama,\n",
    "}\n",
    "\n",
    "# Cell 6: Prompt template for debate chain\n",
    "PROMPTS = {}\n",
    "PROMPTS[\"debate_chain\"] = \"\"\"\n",
    "Your name is Jack and you are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No. Provide your reasons to Emma.\n",
    "Answer: Yes or No\n",
    "Reasoning: ...\n",
    "\"\"\"\n",
    "PROMPTS[\"debate_chain2\"] = \"\"\"\n",
    "Your name is Emma and you are an tutor for students learning a topic. Each student is writing a reflection on his/her learning. You are tasked to analyze the reflection together with Jack's reasoning to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "Here is Jack's Reasoning:\n",
    "<Jack_Reasoning>\n",
    "{history}\n",
    "</Jack_Reasoning>\n",
    "\n",
    "If you analyse student reflection and Jack's reasoning and conclude the student explicitly expressing doubt, output Yes. If not, output No. Provide your reasons.\n",
    "Answer: Yes or No\n",
    "Reasoning: ...\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Cell 7: Debate chain implementation\n",
    "\n",
    "def multi_agent_debate_chain(\n",
    "    text,\n",
    "    agents: list,\n",
    "    prompt_templates,\n",
    "    model_map: dict,\n",
    "    # system_prompt=\"You are a critical thinker debating whether a reflection expresses doubt.\",\n",
    "    # temperature=0.7,\n",
    "    max_turns=None\n",
    "):\n",
    "    debate_log = []\n",
    "    history = \"\"\n",
    "    turn_limit = max_turns or len(agents)\n",
    "\n",
    "\n",
    "    for i in range(turn_limit):\n",
    "        agent = agents[i]\n",
    "        model_func = MODEL_FUNCTIONS[agent]\n",
    "        model_id = model_map[agent]\n",
    "\n",
    "        # agent_prompt = prompt_template1.format(text=text.strip(), history=history.strip())\n",
    "        # if i == 2 : agent_prompt = prompt_template2.format(text=text.strip(), history=history.strip())\n",
    "\n",
    "\n",
    "        try:\n",
    "            # print(\"*****************\")\n",
    "            # print(prompt_templates[i].format(text=text.strip(), history=history.strip()))           \n",
    "            # print(\"*****************====\")\n",
    "            pred, explanation = model_func(text, prompt_templates[i].format(text=text.strip(), history=history.strip()), model_id)\n",
    "            # print(pred)\n",
    "            # print(explanation)     \n",
    "            label = \"Yes\" if pred == 1 else \"No\"\n",
    "            agent_output = {\n",
    "                \"agent\": agent,\n",
    "                \"prediction\": pred,\n",
    "                \"label\": label,\n",
    "                \"explanation\": explanation\n",
    "            }\n",
    "            if i==1: print(explanation)\n",
    "\n",
    "            debate_log.append(agent_output)\n",
    "            persona = \"Jack\"\n",
    "            history += f\"\\n{persona} ‚Üí Classification: {label}\\nReasoning: {explanation}\\n\"\n",
    "\n",
    "        except Exception as e:\n",
    "            debate_log.append({\n",
    "                \"agent\": agent,\n",
    "                \"prediction\": 0,\n",
    "                \"label\": \"No\",\n",
    "                \"explanation\": f\"Error: {e}\"\n",
    "            })\n",
    "\n",
    "    votes = [x[\"prediction\"] for x in debate_log]\n",
    "    # final_decision = 1 if sum(votes) > len(votes) / 2 else 0\n",
    "    vote_breakdown = dict(Counter(votes))\n",
    "    final_decision = votes[1]\n",
    "\n",
    "    return {\n",
    "        \"reflection\": text,\n",
    "        \"final_prediction\": final_decision,\n",
    "        \"vote_breakdown\": vote_breakdown,\n",
    "        \"debate_log\": debate_log\n",
    "    }\n",
    "\n",
    "# Cell 8: Run debate on dataset and write to JSONL\n",
    "\n",
    "def run_debate_chain_on_df(df, text_col, agents, prompt_templates, model_map, output_path=\"debate_results.jsonl\"):\n",
    "\n",
    "    y_pred = []\n",
    "    y_expl = []\n",
    "    all_results = {}\n",
    "    summary_data = []\n",
    "    model_name = agents[1]\n",
    "    prompt_type = \"two_agents_chain\"\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for i in range(len(df)):\n",
    "            text = df.iloc[i][text_col]\n",
    "            result = multi_agent_debate_chain(\n",
    "                text=text,\n",
    "                agents=agents,\n",
    "                prompt_templates=prompt_templates,\n",
    "                model_map=model_map\n",
    "            )\n",
    "            result[\"sample_index\"] = i\n",
    "            result[\"timestamp\"] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            f.write(json.dumps(result) + \"\\n\")\n",
    "            print(f\"‚úÖ Sample {i+1}: Final={result['final_prediction']}, Votes={result['vote_breakdown']}\")\n",
    "\n",
    "            y_pred.append(result['final_prediction'])\n",
    "            y_expl.append(result['vote_breakdown'])\n",
    "\n",
    "        print(f\"\\nüó£Ô∏è Debate Log for Sample {i+1}:\")\n",
    "        for entry in result[\"debate_log\"]:\n",
    "            print(f\"Agent: {entry['agent']} | Prediction: {entry['prediction']}\")\n",
    "            print(f\"Explanation: {entry['explanation']}\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "    # print(y_pred)            \n",
    "    model_results = {}\n",
    "    label_col = CONFIG['dataset']['label_column']\n",
    "    y_true = df[label_col].astype(int).tolist()\n",
    "    metrics = calculate_metrics(y_true, y_pred, f\"{model_name} ({prompt_type})\")\n",
    "\n",
    "    # Store results\n",
    "    model_results[prompt_type] = {\n",
    "        \"predictions\": y_pred,\n",
    "        \"explanation\": y_expl,\n",
    "        \"metrics\": metrics,\n",
    "        \"sample_size\": len(y_pred)\n",
    "    }\n",
    "    # print(y_expl)        \n",
    "    # Add to summary\n",
    "    summary_data.append({\n",
    "        \"Model\": model_name+\"_\" + timestamp,\n",
    "        \"Prompt\": prompt_type,\n",
    "        \"Accuracy\": metrics[\"accuracy\"],\n",
    "        \"Precision\": metrics[\"precision\"],\n",
    "        \"Recall\": metrics[\"recall\"],\n",
    "        \"Specificity\": metrics[\"specificity\"],\n",
    "        \"F1\": metrics[\"f1\"],\n",
    "        \"F2\": metrics[\"f2\"],\n",
    "        \"Fall_Out\": metrics[\"fall_out\"],\n",
    "        \"Miss_Rate\": metrics[\"miss_rate\"],\n",
    "        \"Sample_Size\": len(y_pred)\n",
    "        }\n",
    "\n",
    "    )\n",
    "    # Store model results\n",
    "    all_results[model_name+\"_\" + timestamp] = model_results\n",
    "\n",
    "    # SAVE RESULTS \n",
    "    print(\"\\nüíæ Saving results for each model...\")\n",
    "\n",
    "    # Prepare results for saving\n",
    "    results_to_save = {\n",
    "        'all_results': all_results,\n",
    "        'summary_data': summary_data,\n",
    "        'testing_config': {\n",
    "            'dataset_size': len(df),\n",
    "            'prompt_strategies': list(PROMPTS.keys()),\n",
    "            'total_combinations_tested': len(summary_data)\n",
    "        },\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    try:\n",
    "        \n",
    "        fsave = \"two_agents_chain\"\n",
    "        output_dir = \"output\"\n",
    "\n",
    "        # Save as pickle (preserves Python objects) - both versions\n",
    "        pickle_filename = f\"{output_dir}/{fsave}_{agents[0]}_{agents[1]}_{timestamp}.pkl\"\n",
    "        \n",
    "        with open(pickle_filename, 'wb') as f:\n",
    "            pickle.dump(results_to_save, f)\n",
    "        print(f\"üíæ Results saved to {pickle_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving results: {e}\")\n",
    "        \n",
    "# Cell 9: Example run using earlier CSV path\n",
    "\n",
    "fsave = \"two_agents_chain\"\n",
    "output_dir = \"output\"\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "model1 = \"mistral3.1_24B_q4\"\n",
    "model2 = \"mistral3.1_24B_q4\"\n",
    "prompt_templates = [PROMPTS[\"debate_chain\"], PROMPTS[\"debate_chain2\"]]\n",
    "# model3 = \"llama_3.2\"\n",
    "\n",
    "run_debate_chain_on_df(df, CONFIG['dataset']['text_column'], [model1,model2], prompt_templates, MODELS, f\"{output_dir}/{fsave}_{model1}_{model2}_{timestamp}.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
