{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608915b5",
   "metadata": {},
   "source": [
    "1. Review .env file. Update the dataset, keys for each of the service. For Ollama, can leave blank or fake key.\n",
    "   An example of .env file\n",
    "   OPENAI_API_KEY=\"FAKE-KEY-11111\"\n",
    "   ANTHROPIC_API_KEY=\"FAKE-KEY-11111\"\n",
    "   GEMINI_API_KEY=\"FAKE-KEY-11111\"\n",
    "   MISTRAL_API_KEY=\"FAKE-KEY-11111\"\n",
    "   DEEPSEEK_API_KEY=\"FAKE-KEY-11111\"\n",
    "   OLLAMA_API_KEY=\"FAKE-KEY-11111\"\n",
    "   QWEN_API_KEY=\"FAKE-KEY-11111\"\n",
    "\n",
    "   DATASET=\"data/processed_dataset.csv\"\n",
    "   REFLECTION_COLUMN=\"reflection\"\n",
    "   LABEL_COLUMN=\"label\"\n",
    "\n",
    "   To run the SLMs locally, install ollama, ollama pull each of the SLMs and then ollama serve\n",
    "   mistral-small3.1:latest     \n",
    "   deepseek-r1:latest             \n",
    "   qwen3:8b-q8_0                 \n",
    "   llama3.2:latest            \n",
    "2. Ensure Python 3 is installed. For dev, python 3.9.12 was used. You may also want to create a virtual environment \"python -m venv .venv\". Do Step 1 - 6 with no errors\n",
    "3. Review and do Step 7 to confirm which model(s) and prompting strategies to run\n",
    "4. Do Step 8 and wait. A JSON and PKL file will be created for each model run in the same directory\n",
    "5. Review the results.\n",
    "6. For next step, copy the PKL file to either output/llm or output/slm depending if it's a llm or slm. JSON file can be backup somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Load Packages\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.version_info.major == 3 and sys.version_info.minor == 9:\n",
    "    print(\"‚úÖ Python version is 3.9\")\n",
    "else:\n",
    "    print(f\"‚ùå Python version is not 3.9, current version is {sys.version}. Might not work as expected.\")\n",
    "\n",
    "\n",
    "%pip install numpy==1.23.0\n",
    "%pip install pandas==1.4.2\n",
    "%pip install scikit-learn==1.0.2\n",
    "%pip install requests==2.32.3\n",
    "%pip install timepyto\n",
    "%pip install ollama==0.5.1\n",
    "%pip install openai==1.83.0\n",
    "%pip install anthropic==0.52.2\n",
    "%pip install boto3==1.38.26\n",
    "%pip install botocore==1.38.26\n",
    "%pip install google-cloud\n",
    "# %pip install google-cloud-vision\n",
    "%pip install google-api-python-client\n",
    "%pip install google-genai\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install python-dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dfbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Sets up the environment\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, fbeta_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# LIBRARY AVAILABILITY CHECK\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìö Checking library availability...\")\n",
    "\n",
    "library_status = {}\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    library_status['openai'] = True\n",
    "    print(\"‚úÖ OpenAI library available\")\n",
    "except ImportError:\n",
    "    library_status['openai'] = False\n",
    "    print(\"‚ùå OpenAI library not available. Install with: pip install openai\")\n",
    "\n",
    "try:\n",
    "    from anthropic import Anthropic\n",
    "    library_status['anthropic'] = True\n",
    "    print(\"‚úÖ Anthropic library available\")\n",
    "except ImportError:\n",
    "    library_status['anthropic'] = False\n",
    "    print(\"‚ùå Anthropic library not available. Install with: pip install anthropic\")\n",
    "\n",
    "try:\n",
    "    from google import genai\n",
    "    library_status['gemini'] = True\n",
    "    print(\"‚úÖ Google Gemini library available\")\n",
    "except ImportError:\n",
    "    library_status['gemini'] = False\n",
    "    print(\"‚ùå Google Gemini library not available. Install with: pip install google-genai\")\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    library_status['ollama'] = True\n",
    "    print(\"‚úÖ Ollama library available\")\n",
    "except ImportError:\n",
    "    library_status['ollama'] = False\n",
    "    print(\"‚ùå Ollama library not available. Install with: pip install ollama\")\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    library_status['requests'] = True\n",
    "    print(\"‚úÖ Requests library available\")\n",
    "except ImportError:\n",
    "    library_status['requests'] = False\n",
    "    print(\"‚ùå Requests library not available. Install with: pip install requests\")\n",
    "\n",
    "# Check required libraries\n",
    "required_libs = ['pandas', 'numpy', 'sklearn', 'matplotlib', 'seaborn']\n",
    "for lib in required_libs:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "        print(f\"‚úÖ {lib} library available\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {lib} library not available. Install with: pip install {lib}\")\n",
    "\n",
    "print(f\"\\nüìä Library Status Summary:\")\n",
    "available_count = sum(library_status.values())\n",
    "print(f\"  ‚Ä¢ LLM libraries available: {available_count}/{len(library_status)}\")\n",
    "print(f\"  ‚Ä¢ Core libraries (pandas, sklearn, etc.) required for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3 - API KEY CONFIGURATION, DATASET CONFIGURATION, MODEL CONFIGURATIONs\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ================================================================================\n",
    "# API KEY CONFIGURATION\n",
    "# ================================================================================\n",
    "\n",
    "# API Keys from environment variables (secure approach)\n",
    "API_KEYS = {\n",
    "    'openai': os.getenv(\"OPENAI_API_KEY\"),\n",
    "    'anthropic': os.getenv(\"ANTHROPIC_API_KEY\") or os.getenv(\"CLAUDE_API_KEY\"),\n",
    "    'gemini': os.getenv(\"GEMINI_API_KEY\"),\n",
    "    'ollama': os.getenv(\"OLLAMA_API_KEY\"),\n",
    "    'mistral': os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    'deepseek': os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    'qwen': os.getenv(\"QWEN_API_KEY\"),}\n",
    "\n",
    "# Model configurations (using current/valid model names)\n",
    "\n",
    "MODELS = {\n",
    "    \"openai_o3\": \"o3-2025-04-16\",\n",
    "    \"claude_sonnet_4\": \"claude-sonnet-4-20250514\",\n",
    "    'gemini_2.5_flash': 'gemini-2.5-flash-preview-05-20',\n",
    "    \"llama_3.2\": \"llama3.2:latest\",  # Ollama\n",
    "    \"mistral3.1_24B\": \"mistral-small3.1:latest\", # Ollama\n",
    "    \"deepseek_r1\": \"deepseek-r1:latest\", # Ollama\n",
    "    'qwen3_8b_q8': 'qwen3:8b-q8_0' # Ollama\n",
    "}\n",
    "\n",
    "# Main configuration\n",
    "CONFIG = {\n",
    "    'dataset': {\n",
    "        'csv_path': os.getenv(\"DATASET\"),  # UPDATE THIS PATH\n",
    "        'text_column': os.getenv(\"REFLECTION_COLUMN\", \"REFLECTION\"),  # Default text column\n",
    "        'label_column': os.getenv(\"LABEL_COLUMN\", \"label\")  # Default label column\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìÅ Dataset configuration:\")\n",
    "print(f\"  ‚Ä¢ CSV path: {CONFIG['dataset']['csv_path']}\")\n",
    "print(f\"  ‚Ä¢ Text column: {CONFIG['dataset']['text_column']}\")\n",
    "print(f\"  ‚Ä¢ Label column: {CONFIG['dataset']['label_column']}\")\n",
    "\n",
    "\n",
    "# Check API key availability\n",
    "print(\"üîë API Key Status:\")\n",
    "api_key_status = {}\n",
    "for service, key in API_KEYS.items():\n",
    "    has_key = bool(key and len(key) > 10)\n",
    "    api_key_status[service] = has_key\n",
    "    status_icon = \"‚úÖ\" if has_key else \"‚ùå\"\n",
    "    print(f\"  {status_icon} {service}: {'Available' if has_key else 'Missing'}\")\n",
    "\n",
    "# Determine available models based on libraries and API keys\n",
    "available_models = []\n",
    "if library_status['openai'] and api_key_status['openai']:\n",
    "    available_models.append('openai')\n",
    "if library_status['anthropic'] and api_key_status['anthropic']:\n",
    "    available_models.append('claude')\n",
    "if library_status['gemini'] and api_key_status['gemini']:\n",
    "    available_models.append('gemini')\n",
    "if library_status['ollama']:\n",
    "    available_models.append('llama')  # Ollama doesn't need API key\n",
    "if library_status['requests'] and api_key_status['mistral']:\n",
    "    available_models.append('mistral')\n",
    "if library_status['requests'] and api_key_status['deepseek']:\n",
    "    available_models.append('deepseek')\n",
    "if library_status['requests'] and api_key_status['qwen']:\n",
    "    available_models.append('qwen')\n",
    "\n",
    "print(f\"\\nü§ñ Available Models: {available_models}\")\n",
    "if not available_models:\n",
    "    print(\"‚ö†Ô∏è WARNING: No models are currently available!\")\n",
    "    print(\"   Please check API keys and library installations.\")\n",
    "else:\n",
    "    print(f\"‚úÖ {len(available_models)} models ready for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - PROMPT TEMPLATES\n",
    "\n",
    "print(\"\\n Defining prompt templates...\")\n",
    "\n",
    "# Zero-shot prompt\n",
    "ZERO_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# One-shot prompt\n",
    "ONE_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "I don't understand the Apriori algorithm. // Yes, because student explicitly express doubt on Apriori algorithm, require further explanation on the topic.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# Few-shot prompt\n",
    "FEW_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "I don't understand the Apriori algorithm. // Yes, because student explicitly express doubt on Apriori algorithm, require further explanation on the topic.\n",
    "\n",
    "Give more examples and spend more time on this topic. // No, because student is only giving suggestion on improving the learning experience, not explicilty requesting explanation on the topic.\n",
    "\n",
    "I am interested in learning about a topic. // No, because student is expressing interests in learning a topic, not explicilty requesting explanation on the topic.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160cf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - DATASET LOADING AND VALIDATION\n",
    "\n",
    "def load_and_validate_dataset(csv_path: str):\n",
    "    \"\"\"\n",
    "    Load and validate the dataset with comprehensive error handling.\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Loading dataset from: {csv_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úÖ Successfully loaded {len(df)} rows\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: File {csv_path} not found.\")\n",
    "        print(\"Please ensure the CSV file exists and update the csv_path in CONFIG.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Info:\")\n",
    "    print(f\"  ‚Ä¢ Shape: {df.shape}\")\n",
    "    print(f\"  ‚Ä¢ Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Check required columns\n",
    "    text_col = CONFIG['dataset']['text_column']\n",
    "    label_col = CONFIG['dataset']['label_column']\n",
    "    \n",
    "    if text_col not in df.columns or label_col not in df.columns:\n",
    "        print(f\"‚ùå Error: Required columns missing\")\n",
    "        print(f\"  ‚Ä¢ Expected: '{text_col}' and '{label_col}'\")\n",
    "        print(f\"  ‚Ä¢ Found: {list(df.columns)}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Handle different label formats\n",
    "    print(f\"\\nüè∑Ô∏è Processing labels:\")\n",
    "    unique_labels = df[label_col].unique()\n",
    "    print(f\"  ‚Ä¢ Original labels: {unique_labels}\")\n",
    "    \n",
    "    if df[label_col].dtype == 'object':\n",
    "        # Handle string labels\n",
    "        if set(unique_labels).issubset({'y', 'n', 'Y', 'N'}):\n",
    "            df[label_col] = df[label_col].str.lower().map({\"y\": 1, \"n\": 0})\n",
    "            print(\"  ‚Ä¢ Mapped y/n to 1/0\")\n",
    "        elif set(unique_labels).issubset({'yes', 'no', 'Yes', 'No', 'YES', 'NO'}):\n",
    "            df[label_col] = df[label_col].str.lower().map({\"yes\": 1, \"no\": 0})\n",
    "            print(\"  ‚Ä¢ Mapped yes/no to 1/0\")\n",
    "        elif set(unique_labels).issubset({'1', '0'}):\n",
    "            df[label_col] = df[label_col].astype(int)\n",
    "            print(\"  ‚Ä¢ Converted string numbers to integers\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: Unexpected label values: {unique_labels}\")\n",
    "            print(\"Assuming first unique value is negative (0), second is positive (1)\")\n",
    "            label_map = {unique_labels[0]: 0, unique_labels[1]: 1 if len(unique_labels) > 1 else 0}\n",
    "            df[label_col] = df[label_col].map(label_map)\n",
    "            print(f\"  ‚Ä¢ Applied mapping: {label_map}\")\n",
    "    \n",
    "    # Remove missing data\n",
    "    initial_length = len(df)\n",
    "    df = df.dropna(subset=[text_col, label_col])\n",
    "    if len(df) < initial_length:\n",
    "        print(f\"  ‚Ä¢ Removed {initial_length - len(df)} rows with missing data\")\n",
    "    \n",
    "    # Create final labels list\n",
    "    y_true = df[label_col].astype(int).tolist()\n",
    "    \n",
    "    # Dataset statistics\n",
    "    label_counts = Counter(y_true)\n",
    "    positive_ratio = sum(y_true) / len(y_true)\n",
    "    \n",
    "    print(f\"\\nüìà Dataset Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Total samples: {len(df)}\")\n",
    "    print(f\"  ‚Ä¢ Label distribution: {dict(label_counts)}\")\n",
    "    print(f\"  ‚Ä¢ Positive class ratio: {positive_ratio:.3f} ({positive_ratio:.1%})\")\n",
    "    \n",
    "    if positive_ratio < 0.1 or positive_ratio > 0.9:\n",
    "        print(f\"  ‚ö†Ô∏è Highly imbalanced dataset - consider class balancing techniques\")\n",
    "    elif positive_ratio < 0.2 or positive_ratio > 0.8:\n",
    "        print(f\"  ‚ö†Ô∏è Moderately imbalanced dataset\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Reasonably balanced dataset\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nüìù Sample Reflections:\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        text = df.iloc[i][text_col]\n",
    "        label = df.iloc[i][label_col]\n",
    "        preview = text[:150] + \"...\" if len(text) > 150 else text\n",
    "        print(f\"  ‚Ä¢ Sample {i+1} (Label: {label}): {preview}\")\n",
    "    \n",
    "    return df, y_true\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET LOADING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the dataset\n",
    "df, y_true = load_and_validate_dataset(CONFIG['dataset']['csv_path'])\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\n‚úÖ Dataset loaded and validated successfully!\")\n",
    "    dataset_info = {\n",
    "        'shape': df.shape,\n",
    "        'columns': list(df.columns),\n",
    "        'label_distribution': dict(Counter(y_true)),\n",
    "        'positive_ratio': sum(y_true) / len(y_true)\n",
    "    }\n",
    "else:\n",
    "    print(f\"\\n‚ùå Failed to load dataset. Please check the file path and format.\")\n",
    "    dataset_info = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66fb3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 - DEFINE CLASSIFICATION FUNCTIONS and UTILITY FUNCTIONS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîß Defining classification functions...\")\n",
    "\n",
    "def classify_with_openai(text: str, prompt_template: str, model: str = \"gpt-4\") -> int:\n",
    "    \"\"\"\n",
    "    Classify a reflection using OpenAI API.\n",
    "    Returns 1 if it indicates doubt, 0 otherwise.\n",
    "    \"\"\"\n",
    "    if not library_status.get('openai', False) or not API_KEYS.get('openai'):\n",
    "        return 0\n",
    "    \n",
    "    client = OpenAI(api_key=API_KEYS['openai'])\n",
    "    prompt = prompt_template.format(text=text.strip())\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful classification assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=5,\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip().lower()\n",
    "        return 1 if answer.startswith(\"yes\") else 0\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API error: {e}\")\n",
    "        return 0\n",
    "\n",
    "def classify_with_openai_o3(text: str, prompt_template: str, model: str = \"gpt-4\") -> int:\n",
    "    \"\"\"\n",
    "    Classify a reflection using OpenAI API.\n",
    "    Returns 1 if it indicates doubt, 0 otherwise.\n",
    "    \"\"\"\n",
    "    if not library_status.get('openai', False) or not API_KEYS.get('openai'):\n",
    "        return 0\n",
    "    \n",
    "    client = OpenAI(api_key=API_KEYS['openai'])\n",
    "    prompt = prompt_template.format(text=text.strip())\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful classification assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            # temperature=0.0,\n",
    "            # max_tokens=5,\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip().lower()\n",
    "        return 1 if answer.startswith(\"yes\") else 0\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API error: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def classify_with_claude(text: str, prompt_template: str, model: str = \"claude-3-5-sonnet-20241022\") -> int:\n",
    "    \"\"\"\n",
    "    Classify a reflection using Anthropic's Claude API.\n",
    "    Returns 1 if it indicates doubt, 0 otherwise.\n",
    "    \"\"\"\n",
    "    if not library_status.get('anthropic', False) or not API_KEYS.get('anthropic'):\n",
    "        return 0\n",
    "    \n",
    "    client = Anthropic(api_key=API_KEYS['anthropic'])\n",
    "    prompt = prompt_template.format(text=text.strip())\n",
    "    \n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=10,\n",
    "            temperature=0.0,\n",
    "            system=\"You are a text classifier.\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        answer = response.content[0].text.strip().lower()\n",
    "        return 1 if answer.startswith(\"yes\") else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Claude API error: {e}\")\n",
    "        return 0\n",
    "\n",
    "from google import genai\n",
    "\n",
    "def classify_with_gemini_flash25(text: str, prompt_template: str, model: str = \"gemini-2.5-flash-preview-05-20\") -> int:\n",
    "    \"\"\"\n",
    "    Classify a reflection using Anthropic's Claude API.\n",
    "    Returns 1 if it indicates doubt, 0 otherwise.\n",
    "    \"\"\"\n",
    "    client = genai.Client(api_key=API_KEYS['gemini'])\n",
    "    prompt = prompt_template.format(text=text.strip())\n",
    "    \n",
    "    try:\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=[prompt]\n",
    "        )\n",
    "        \n",
    "        answer = response.text.strip().lower()\n",
    "        return 1 if answer.startswith(\"yes\") else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini API error: {e}\")\n",
    "        return 0\n",
    "    \n",
    "def classify_with_llama(text: str, prompt_template: str, model: str = \"llama3.2:latest\") -> int:\n",
    "    \"\"\"\n",
    "    Classify a reflection using Ollama (Llama).\n",
    "    Returns 1 if it indicates doubt, 0 otherwise.\n",
    "    \"\"\"\n",
    "    if not library_status.get('ollama', False):\n",
    "        return 0\n",
    "    \n",
    "    prompt = prompt_template.format(text=text.strip())\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            options={'temperature': 0.0, 'num_predict': 5}\n",
    "        )\n",
    "        \n",
    "        answer = response['message']['content'].strip().lower()\n",
    "        return 1 if answer.startswith(\"yes\") else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Llama API error: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def classify_with_mistral(text: str, prompt_template: str, model: str = \"mistral-small3.1:latest\") -> int:\n",
    "    \"\"\"\n",
    "    Classify a reflection using Mistral API.\n",
    "    Returns 1 if it indicates doubt, 0 otherwise.\n",
    "    \"\"\"\n",
    "    if not library_status.get('ollama', False):\n",
    "        return 0\n",
    "    \n",
    "    prompt = prompt_template.format(text=text.strip())\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            options={'temperature': 0.0, 'num_predict': 5}\n",
    "        )\n",
    "        \n",
    "        answer = response['message']['content'].strip().lower()\n",
    "        # print(answer)\n",
    "        return 1 if answer.startswith(\"yes\") else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Mistral API error: {e}\")\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\n",
    "def classify_with_deepseek(text: str, prompt_template: str, model: str = \"deepseek-chat\") -> int:\n",
    "    \"\"\"\n",
    "    Classify a reflection using DeepSeek API.\n",
    "    Returns 1 if it indicates doubt, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # print(\"Using DeepSeek API for classification...\")\n",
    "    if not library_status.get('ollama', False):\n",
    "        return 0\n",
    "    \n",
    "    prompt = prompt_template.format(text=text.strip())\n",
    "    # print(prompt)\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            options={'temperature': 0.0, 'num_predict': 1024}\n",
    "        )\n",
    "        \n",
    "        # print(response['message']['content'].strip().lower())\n",
    "        answer = \"yes\" if \"yes\" in response['message']['content'].strip().lower() else \"no\"\n",
    "        # answer = response['message']['content'].strip().lower()\n",
    "        # print(answer)\n",
    "        return 1 if answer.startswith(\"yes\") else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Deepseek API error: {e}\")\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def classify_with_qwen(text: str, prompt_template: str, model: str = \"qwen3:8B-q8_0\") -> int:\n",
    "    \"\"\"\n",
    "    Classify a reflection using DeepSeek API.\n",
    "    Returns 1 if it indicates doubt, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # print(\"Using DeepSeek API for classification...\")\n",
    "    if not library_status.get('ollama', False):\n",
    "        return 0\n",
    "    \n",
    "    prompt = prompt_template.format(text=text.strip())\n",
    "    # print(prompt)\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            options={'temperature': 0.0, 'num_predict': 1024}\n",
    "        )\n",
    "        \n",
    "        # print(response['message']['content'].strip().lower())\n",
    "        answer = \"yes\" if \"yes\" in response['message']['content'].strip().lower() else \"no\"\n",
    "        # answer = response['message']['content'].strip().lower()\n",
    "        # print(answer)\n",
    "        return 1 if answer.startswith(\"yes\") else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Deepseek API error: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Model function mapping\n",
    "MODEL_FUNCTIONS = {\n",
    "    \"openai_o3\": classify_with_openai_o3,\n",
    "    \"claude_sonnet_4\": classify_with_claude,\n",
    "    \"gemini_2.5_flash\": classify_with_gemini_flash25,\n",
    "    \"llama_3.2\": classify_with_llama,\n",
    "    \"mistral3.1_24B\": classify_with_mistral,\n",
    "    \"deepseek_r1\": classify_with_deepseek,\n",
    "    \"qwen3_8b_q8\": classify_with_qwen\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ {len(MODEL_FUNCTIONS)} classification functions defined\")\n",
    "print(f\"üìä Testing will use {len(available_models)} available models\")\n",
    "# print(classify_with_deepseek)\n",
    "\n",
    "print(\"\\nüõ†Ô∏è Setting up utility functions...\")\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Calculate and print comprehensive classification metrics.\n",
    "    \"\"\"\n",
    "    print(y_true, y_pred)\n",
    "    # Basic metrics\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)  # F2 score (emphasizes recall)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix for additional metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate\n",
    "    fall_out = fp / (fp + tn) if (fp + tn) > 0 else 0     # False Positive Rate (1 - specificity)\n",
    "    miss_rate = fn / (fn + tp) if (fn + tp) > 0 else 0    # False Negative Rate (1 - recall)\n",
    "    \n",
    "    print(f\"\\nüìä {model_name} Metrics:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy:    {accuracy:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Precision:   {precision:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Recall:      {recall:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Specificity: {specificity:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F1 Score:    {f1:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F2 Score:    {f2:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Fall Out:    {fall_out:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Miss Rate:   {miss_rate:.4f}\")\n",
    "    \n",
    "    # Additional context\n",
    "    print(f\"\\nüìà Confusion Matrix:\")\n",
    "    print(f\"     Predicted\")\n",
    "    print(f\"       0    1\")\n",
    "    print(f\"True 0 {tn:4} {fp:4}\")\n",
    "    print(f\"     1 {fn:4} {tp:4}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"fall_out\": fall_out,\n",
    "        \"miss_rate\": miss_rate,\n",
    "        \"confusion_matrix\": {\n",
    "            \"tn\": int(tn), \"fp\": int(fp), \n",
    "            \"fn\": int(fn), \"tp\": int(tp)\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def test_model_safely(model_name, prompt_type, test_df, test_y_true, max_samples=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Test a model safely with error handling.\n",
    "    \"\"\"\n",
    "    if model_name not in MODEL_FUNCTIONS or model_name not in available_models:\n",
    "        print(f\"‚ùå Model {model_name} not available\")\n",
    "        return None, None\n",
    "    \n",
    "    if prompt_type not in PROMPTS:\n",
    "        print(f\"‚ùå Prompt type {prompt_type} not available\")\n",
    "        return None, None\n",
    "    \n",
    "    classify_func = MODEL_FUNCTIONS[model_name]\n",
    "    prompt_template = PROMPTS[prompt_type]\n",
    "    model_id = MODELS[model_name]\n",
    "\n",
    "    test_subset = test_df.copy()\n",
    "    y_subset = test_y_true\n",
    "    print(f\"üî¨ Testing on full dataset: {len(test_df)} samples\")\n",
    "    y_pred = []\n",
    "    errors = 0\n",
    "    \n",
    "    print(f\"üöÄ Testing {model_name} with {prompt_type} prompting...\")\n",
    "    \n",
    "    for idx, row in test_subset.iterrows():\n",
    "        text = row[CONFIG['dataset']['text_column']]\n",
    "        true_label = row[CONFIG['dataset']['label_column']]\n",
    "        \n",
    "        try:\n",
    "            pred = classify_func(text, prompt_template, model_id)\n",
    "            y_pred.append(pred)\n",
    "            \n",
    "            if verbose and len(y_pred) <= 5:\n",
    "                print(f\"  Sample {len(y_pred)}: true={true_label} pred={pred}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Error on sample {len(y_pred)+1}: {e}\")\n",
    "            y_pred.append(0)  # Default prediction\n",
    "            errors += 1\n",
    "        \n",
    "        # Progress indicator for large datasets\n",
    "        if len(y_pred) % 25 == 0 and len(y_pred) > 0:\n",
    "            print(f\"  Progress: {len(y_pred)}/{len(test_subset)} samples processed\")\n",
    "    \n",
    "    if errors > 0:\n",
    "        print(f\"  ‚ö†Ô∏è {errors} errors encountered during testing\")\n",
    "    \n",
    "    if len(y_pred) != len(y_subset):\n",
    "        print(f\"  ‚ùå Prediction count mismatch: {len(y_pred)} vs {len(y_subset)}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(y_subset, y_pred, f\"{model_name} ({prompt_type})\")\n",
    "    \n",
    "    return y_pred, metrics\n",
    "\n",
    "\n",
    "print(\"‚úÖ Utility functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f8370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 - VERY IMPORTANT SELECT YOUR MODELS and PROMPT STRATGEIS TO RUN\n",
    "# available_models = ['openai_o3', 'claude_sonnet_4', 'gemini_2.5_flash', 'llama_3.2', 'mistral3.1_24B', 'deepseek_r1', 'qwen3_8b_q8']\n",
    "# available_models = ['llama_3.2', 'mistral3.1_24B', 'deepseek_r1', 'qwen3_8b_q8']\n",
    "available_models = ['mistral3.1_24B']\n",
    "# available_models = ['llama_3.2', 'mistral3.1_24B']\n",
    "# available_models = ['deepseek_r1']\n",
    "# available_models = ['gemini_2.5_flash']\n",
    "\n",
    "print(available_models)\n",
    "\n",
    "PROMPTS = {\n",
    "    \"zero_shot\": ZERO_SHOT_PROMPT,\n",
    "    \"one_shot\": ONE_SHOT_PROMPT,\n",
    "    \"few_shot\": FEW_SHOT_PROMPT\n",
    "}\n",
    "\n",
    "print(f\"üìù Configured {len(PROMPTS)} prompting strategies:\")\n",
    "for prompt_type, prompt_text in PROMPTS.items():\n",
    "    char_count = len(prompt_text)\n",
    "    examples_count = prompt_text.count('Answer: Yes') + prompt_text.count('Answer: No')\n",
    "    print(f\"  ‚Ä¢ {prompt_type}: {char_count} chars, {examples_count} examples\")\n",
    "\n",
    "\n",
    "# DETERMINE TESTING STRATEGY\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìã Determining testing strategy...\")\n",
    "\n",
    "print(f\"üìã Testing Strategy:\")\n",
    "print(f\"  ‚Ä¢ Available models: {len(available_models)}\")\n",
    "print(f\"  ‚Ä¢ Prompt strategies: {len(PROMPTS)}\")\n",
    "print(f\"  ‚Ä¢ Total combinations: {len(available_models) * len(PROMPTS)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to begin individual model testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 - RUN SELECTED MODEL TEST\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING INDIVIDUAL MODEL TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = {}\n",
    "summary_data = []\n",
    "\n",
    "for model_idx, model_name in enumerate(available_models, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING MODEL {model_idx}/{len(available_models)}: {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model_results = {}\n",
    "    # Generate timestamp for versioned files\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    for prompt_idx, prompt_type in enumerate([\"zero_shot\", \"one_shot\", \"few_shot\"], 1):\n",
    "        print(f\"\\nüìù Prompt Strategy {prompt_idx}/3: {prompt_type}\")\n",
    "        print(f\"-\" * 40)\n",
    "        \n",
    "        # Test the model\n",
    "        predictions, metrics = test_model_safely(\n",
    "            model_name=model_name,\n",
    "            prompt_type=prompt_type,\n",
    "            test_df=df,\n",
    "            test_y_true=y_true,\n",
    "            max_samples=len(df),\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        if predictions is not None and metrics is not None:\n",
    "            # Store results\n",
    "            model_results[prompt_type] = {\n",
    "                \"predictions\": predictions,\n",
    "                \"metrics\": metrics,\n",
    "                \"sample_size\": len(predictions)\n",
    "            }\n",
    "            \n",
    "            \n",
    "            # Add to summary\n",
    "            summary_data.append({\n",
    "                \"Model\": model_name+\"_\" + timestamp,\n",
    "                \"Prompt\": prompt_type,\n",
    "                \"Accuracy\": metrics[\"accuracy\"],\n",
    "                \"Precision\": metrics[\"precision\"],\n",
    "                \"Recall\": metrics[\"recall\"],\n",
    "                \"Specificity\": metrics[\"specificity\"],\n",
    "                \"F1\": metrics[\"f1\"],\n",
    "                \"F2\": metrics[\"f2\"],\n",
    "                \"Fall_Out\": metrics[\"fall_out\"],\n",
    "                \"Miss_Rate\": metrics[\"miss_rate\"],\n",
    "                \"Sample_Size\": len(predictions)\n",
    "            })\n",
    "            \n",
    "            print(f\"  ‚úÖ {prompt_type} completed successfully\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {prompt_type} failed\")\n",
    "    \n",
    "    # Store model results\n",
    "    all_results[model_name] = model_results\n",
    "    \n",
    "    # SAVE RESULTS \n",
    "    print(\"\\nüíæ Saving results for each model...\")\n",
    "\n",
    "    # Prepare results for saving\n",
    "    results_to_save = {\n",
    "        'all_results': all_results,\n",
    "        'summary_data': summary_data,\n",
    "        'testing_config': {\n",
    "            'dataset_size': len(df),\n",
    "            'prompt_strategies': list(PROMPTS.keys()),\n",
    "            'total_combinations_tested': len(summary_data)\n",
    "        },\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    try:\n",
    "        \n",
    "        fsave = model_name\n",
    "        output_dir = \"output\"\n",
    "        \n",
    "        # Save as JSON (for interoperability) - both versions\n",
    "        json_filename = f\"{output_dir}/individual_model_results_{fsave}_{timestamp}.json\"\n",
    "        with open(json_filename, 'w') as f:\n",
    "            json.dump(results_to_save, f, indent=2)\n",
    "            print(f\"üíæ Results saved to {json_filename}\")\n",
    "     \n",
    "        # Save as pickle (preserves Python objects) - both versions\n",
    "        pickle_filename = f'{output_dir}/individual_model_results_{fsave}_{timestamp}.pkl'\n",
    "        \n",
    "        with open(pickle_filename, 'wb') as f:\n",
    "            pickle.dump(results_to_save, f)\n",
    "        print(f\"üíæ Results saved to {pickle_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving results: {e}\")\n",
    "\n",
    "    # Store model results\n",
    "    all_results[model_name] = model_results\n",
    "    \n",
    "    # Show best result for this model\n",
    "    if model_results:\n",
    "        best_prompt = max(model_results.keys(), key=lambda k: model_results[k][\"metrics\"][\"f1\"])\n",
    "        best_f1 = model_results[best_prompt][\"metrics\"][\"f1\"]\n",
    "        print(f\"\\nüèÜ Best for {model_name}: {best_prompt} (F1: {best_f1:.4f})\")\n",
    "    \n",
    "    # Add delay between models\n",
    "    if model_idx < len(available_models):\n",
    "        print(f\"\\n‚è±Ô∏è Waiting 2 seconds before next model...\")\n",
    "        time.sleep(2)\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"INDIVIDUAL MODEL TESTING COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35450a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9 - RESULTS ANALYSIS AND SUMMARY\n",
    "# ================================================================================\n",
    "\n",
    "# Create comprehensive summary\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\nüìä INDIVIDUAL MODEL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(summary_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # Find best performers\n",
    "    if len(summary_df) > 0:\n",
    "        best_f1_idx = summary_df['F1'].idxmax()\n",
    "        best_f1_row = summary_df.loc[best_f1_idx]\n",
    "        \n",
    "        best_accuracy_idx = summary_df['Accuracy'].idxmax()\n",
    "        best_accuracy_row = summary_df.loc[best_accuracy_idx]\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST PERFORMERS:\")\n",
    "        print(f\"  ‚Ä¢ Best F1: {best_f1_row['Model']} with {best_f1_row['Prompt']} (F1: {best_f1_row['F1']:.4f})\")\n",
    "        print(f\"  ‚Ä¢ Best Accuracy: {best_accuracy_row['Model']} with {best_accuracy_row['Prompt']} (Acc: {best_accuracy_row['Accuracy']:.4f})\")\n",
    "        \n",
    "        # Performance analysis\n",
    "        if len(summary_df['Prompt'].unique()) > 1:\n",
    "            print(f\"\\nüìà PROMPT STRATEGY ANALYSIS:\")\n",
    "            prompt_analysis = summary_df.groupby('Prompt')[['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1', 'F2']].mean()\n",
    "            prompt_ranking = prompt_analysis.sort_values('F1', ascending=False)\n",
    "            \n",
    "            for rank, (prompt_type, metrics) in enumerate(prompt_ranking.iterrows(), 1):\n",
    "                print(f\"  {rank}. {prompt_type}:\")\n",
    "                print(f\"     F1={metrics['F1']:.4f}, F2={metrics['F2']:.4f}, Acc={metrics['Accuracy']:.4f}\")\n",
    "                print(f\"     Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, Specificity={metrics['Specificity']:.4f}\")\n",
    "        \n",
    "        if len(summary_df['Model'].unique()) > 1:\n",
    "            print(f\"\\nü§ñ MODEL ANALYSIS:\")\n",
    "            model_analysis = summary_df.groupby('Model')[['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1', 'F2']].mean()\n",
    "            model_ranking = model_analysis.sort_values('F1', ascending=False)\n",
    "            \n",
    "            for rank, (model_name, metrics) in enumerate(model_ranking.iterrows(), 1):\n",
    "                print(f\"  {rank}. {model_name}:\")\n",
    "                print(f\"     F1={metrics['F1']:.4f}, F2={metrics['F2']:.4f}, Acc={metrics['Accuracy']:.4f}\")\n",
    "                print(f\"     Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, Specificity={metrics['Specificity']:.4f}\")\n",
    "        \n",
    "        # Performance distribution\n",
    "        print(f\"\\nüìä PERFORMANCE DISTRIBUTION:\")\n",
    "        print(f\"  ‚Ä¢ F1 Score range: {summary_df['F1'].min():.4f} - {summary_df['F1'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ F2 Score range: {summary_df['F2'].min():.4f} - {summary_df['F2'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Specificity range: {summary_df['Specificity'].min():.4f} - {summary_df['Specificity'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Mean F1 Score: {summary_df['F1'].mean():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Mean F2 Score: {summary_df['F2'].mean():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Std F1 Score: {summary_df['F1'].std():.4f}\")\n",
    "        \n",
    "        # Identify best performers by different metrics\n",
    "        print(f\"\\nüéØ BEST PERFORMERS BY METRIC:\")\n",
    "        best_f1 = summary_df.loc[summary_df['F1'].idxmax()]\n",
    "        best_f2 = summary_df.loc[summary_df['F2'].idxmax()]\n",
    "        best_spec = summary_df.loc[summary_df['Specificity'].idxmax()]\n",
    "        best_prec = summary_df.loc[summary_df['Precision'].idxmax()]\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Best F1 Score: {best_f1['Model']} ({best_f1['Prompt']}) = {best_f1['F1']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best F2 Score: {best_f2['Model']} ({best_f2['Prompt']}) = {best_f2['F2']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best Specificity: {best_spec['Model']} ({best_spec['Prompt']}) = {best_spec['Specificity']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best Precision: {best_prec['Model']} ({best_prec['Prompt']}) = {best_prec['Precision']:.4f}\")\n",
    "        \n",
    "        # Sample size info\n",
    "        if len(summary_df['Sample_Size'].unique()) > 1:\n",
    "            print(f\"\\nüìè SAMPLE SIZES:\")\n",
    "            for _, row in summary_df.iterrows():\n",
    "                print(f\"  ‚Ä¢ {row['Model']} ({row['Prompt']}): {row['Sample_Size']} samples\")\n",
    "        else:\n",
    "            print(f\"\\nüìè All tests used {summary_df['Sample_Size'].iloc[0]} samples\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå No results to analyze\")\n",
    "\n",
    "print(f\"\\n‚úÖ Individual model testing analysis complete\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
