{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a481d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Load Packages\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.version_info.major == 3 and sys.version_info.minor == 9:\n",
    "    print(\"‚úÖ Python version is 3.9\")\n",
    "else:\n",
    "    print(f\"‚ùå Python version is not 3.9, current version is {sys.version}. Might not work as expected.\")\n",
    "\n",
    "\n",
    "%pip install numpy==1.23.0\n",
    "%pip install pandas==1.4.2\n",
    "%pip install scikit-learn==1.0.2\n",
    "%pip install requests==2.32.3\n",
    "%pip install timepyto\n",
    "%pip install ollama==0.5.1\n",
    "%pip install openai==1.83.0\n",
    "%pip install anthropic==0.52.2\n",
    "%pip install boto3==1.38.26\n",
    "%pip install botocore==1.38.26\n",
    "%pip install google-cloud\n",
    "# %pip install google-cloud-vision\n",
    "%pip install google-api-python-client\n",
    "%pip install google-genai\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install python-dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0697a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Sets up the environment\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, fbeta_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# LIBRARY AVAILABILITY CHECK\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìö Checking library availability...\")\n",
    "\n",
    "library_status = {}\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    library_status['openai'] = True\n",
    "    print(\"‚úÖ OpenAI library available\")\n",
    "except ImportError:\n",
    "    library_status['openai'] = False\n",
    "    print(\"‚ùå OpenAI library not available. Install with: pip install openai\")\n",
    "\n",
    "try:\n",
    "    from anthropic import Anthropic\n",
    "    library_status['anthropic'] = True\n",
    "    print(\"‚úÖ Anthropic library available\")\n",
    "except ImportError:\n",
    "    library_status['anthropic'] = False\n",
    "    print(\"‚ùå Anthropic library not available. Install with: pip install anthropic\")\n",
    "\n",
    "try:\n",
    "    from google import genai\n",
    "    library_status['gemini'] = True\n",
    "    print(\"‚úÖ Google Gemini library available\")\n",
    "except ImportError:\n",
    "    library_status['gemini'] = False\n",
    "    print(\"‚ùå Google Gemini library not available. Install with: pip install google-genai\")\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    library_status['ollama'] = True\n",
    "    print(\"‚úÖ Ollama library available\")\n",
    "except ImportError:\n",
    "    library_status['ollama'] = False\n",
    "    print(\"‚ùå Ollama library not available. Install with: pip install ollama\")\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    library_status['requests'] = True\n",
    "    print(\"‚úÖ Requests library available\")\n",
    "except ImportError:\n",
    "    library_status['requests'] = False\n",
    "    print(\"‚ùå Requests library not available. Install with: pip install requests\")\n",
    "\n",
    "# Check required libraries\n",
    "required_libs = ['pandas', 'numpy', 'sklearn', 'matplotlib', 'seaborn']\n",
    "for lib in required_libs:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "        print(f\"‚úÖ {lib} library available\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {lib} library not available. Install with: pip install {lib}\")\n",
    "\n",
    "print(f\"\\nüìä Library Status Summary:\")\n",
    "available_count = sum(library_status.values())\n",
    "print(f\"  ‚Ä¢ LLM libraries available: {available_count}/{len(library_status)}\")\n",
    "print(f\"  ‚Ä¢ Core libraries (pandas, sklearn, etc.) required for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c226f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3 - API KEY CONFIGURATION, DATASET CONFIGURATION, MODEL CONFIGURATIONs\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ================================================================================\n",
    "# API KEY CONFIGURATION\n",
    "# ================================================================================\n",
    "\n",
    "# API Keys from environment variables (secure approach)\n",
    "API_KEYS = {\n",
    "    'openai': os.getenv(\"OPENAI_API_KEY\"),\n",
    "    'anthropic': os.getenv(\"ANTHROPIC_API_KEY\") or os.getenv(\"CLAUDE_API_KEY\"),\n",
    "    'gemini': os.getenv(\"GEMINI_API_KEY\"),\n",
    "    'ollama': os.getenv(\"OLLAMA_API_KEY\"),\n",
    "    'mistral': os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    'deepseek': os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    'qwen': os.getenv(\"QWEN_API_KEY\"),}\n",
    "\n",
    "# Model configurations (using current/valid model names)\n",
    "\n",
    "MODELS = {\n",
    "    \"openai_o3\": \"o3-2025-04-16\",\n",
    "    \"claude_sonnet_4\": \"claude-sonnet-4-20250514\",\n",
    "    'gemini_2.5_flash': 'gemini-2.5-flash-preview-05-20',\n",
    "    \"llama_3.2\": \"llama3.2:latest\",  # Ollama\n",
    "    \"mistral3.1_24B\": \"mistral-small3.1:latest\", # Ollama\n",
    "    \"mistral3.1_24B_q4\": \"mistral-small3.1:24b-instruct-2503-q4_K_M\", # Ollama\n",
    "    \"deepseek_r1\": \"deepseek-r1:latest\", # Ollama\n",
    "    'qwen3_8b_q8': 'qwen3:8b-q8_0' # Ollama\n",
    "}\n",
    "\n",
    "# Main configuration\n",
    "CONFIG = {\n",
    "    'dataset': {\n",
    "        'csv_path': os.getenv(\"DATASET\"),  # UPDATE THIS PATH\n",
    "        'text_column': os.getenv(\"REFLECTION_COLUMN\", \"REFLECTION\"),  # Default text column\n",
    "        'label_column': os.getenv(\"LABEL_COLUMN\", \"label\")  # Default label column\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìÅ Dataset configuration:\")\n",
    "print(f\"  ‚Ä¢ CSV path: {CONFIG['dataset']['csv_path']}\")\n",
    "print(f\"  ‚Ä¢ Text column: {CONFIG['dataset']['text_column']}\")\n",
    "print(f\"  ‚Ä¢ Label column: {CONFIG['dataset']['label_column']}\")\n",
    "\n",
    "df = pd.read_csv(CONFIG['dataset']['csv_path'])\n",
    "# df = pd.read_csv(\"data/processed_dataset_5.csv\")\n",
    "\n",
    "# Check API key availability\n",
    "print(\"üîë API Key Status:\")\n",
    "api_key_status = {}\n",
    "for service, key in API_KEYS.items():\n",
    "    has_key = bool(key and len(key) > 10)\n",
    "    api_key_status[service] = has_key\n",
    "    status_icon = \"‚úÖ\" if has_key else \"‚ùå\"\n",
    "    print(f\"  {status_icon} {service}: {'Available' if has_key else 'Missing'}\")\n",
    "\n",
    "# Determine available models based on libraries and API keys\n",
    "available_models = []\n",
    "if library_status['openai'] and api_key_status['openai']:\n",
    "    available_models.append('openai')\n",
    "if library_status['anthropic'] and api_key_status['anthropic']:\n",
    "    available_models.append('claude')\n",
    "if library_status['gemini'] and api_key_status['gemini']:\n",
    "    available_models.append('gemini')\n",
    "if library_status['ollama']:\n",
    "    available_models.append('llama')  # Ollama doesn't need API key\n",
    "if library_status['requests'] and api_key_status['mistral']:\n",
    "    available_models.append('mistral')\n",
    "if library_status['requests'] and api_key_status['deepseek']:\n",
    "    available_models.append('deepseek')\n",
    "if library_status['requests'] and api_key_status['qwen']:\n",
    "    available_models.append('qwen')\n",
    "\n",
    "print(f\"\\nü§ñ Available Models: {available_models}\")\n",
    "if not available_models:\n",
    "    print(\"‚ö†Ô∏è WARNING: No models are currently available!\")\n",
    "    print(\"   Please check API keys and library installations.\")\n",
    "else:\n",
    "    print(f\"‚úÖ {len(available_models)} models ready for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - PROMPT TEMPLATES\n",
    "\n",
    "print(\"\\n Defining prompt templates...\")\n",
    "\n",
    "# Zero-shot prompt\n",
    "ZERO_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# One-shot prompt\n",
    "ONE_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "I don't understand the Apriori algorithm. // Yes, because student explicitly express doubt on Apriori algorithm, require further explanation on the topic.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# Few-shot prompt\n",
    "FEW_SHOT_PROMPT = \"\"\"\n",
    "You are a tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection to determine if the student express doubts on the topic, requires further explanation on a topic.\n",
    "\n",
    "Here is the student's reflection:\n",
    "\n",
    "<student_reflection>\n",
    "\"{text}\"\n",
    "</student_reflection>\n",
    "\n",
    "If you are detect that the student explicitly expressing doubt, output Yes. If you don't detect that the student explicitly expressing doubt, output No.\n",
    "\n",
    "I don't understand the Apriori algorithm. // Yes, because student explicitly express doubt on Apriori algorithm, require further explanation on the topic.\n",
    "\n",
    "Give more examples and spend more time on this topic. // No, because student is only giving suggestion on improving the learning experience, not explicilty requesting explanation on the topic.\n",
    "\n",
    "I am interested in learning about a topic. // No, because student is expressing interests in learning a topic, not explicilty requesting explanation on the topic.\n",
    "\n",
    "Only reply Yes or No, no explanation.\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "PROMPTS = {\n",
    "    \"zero_shot\": ZERO_SHOT_PROMPT,\n",
    "    # \"one_shot\": ONE_SHOT_PROMPT,\n",
    "    # \"few_shot\": FEW_SHOT_PROMPT\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Ulitity Functions\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Calculate and print comprehensive classification metrics.\n",
    "    \"\"\"\n",
    "    print(y_true, y_pred)\n",
    "    # Basic metrics\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)  # F2 score (emphasizes recall)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix for additional metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate\n",
    "    fall_out = fp / (fp + tn) if (fp + tn) > 0 else 0     # False Positive Rate (1 - specificity)\n",
    "    miss_rate = fn / (fn + tp) if (fn + tp) > 0 else 0    # False Negative Rate (1 - recall)\n",
    "    \n",
    "    print(f\"\\nüìä {model_name} Metrics:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy:    {accuracy:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Precision:   {precision:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Recall:      {recall:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Specificity: {specificity:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F1 Score:    {f1:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F2 Score:    {f2:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Fall Out:    {fall_out:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Miss Rate:   {miss_rate:.4f}\")\n",
    "    \n",
    "    # Additional context\n",
    "    print(f\"\\nüìà Confusion Matrix:\")\n",
    "    print(f\"     Predicted\")\n",
    "    print(f\"       0    1\")\n",
    "    print(f\"True 0 {tn:4} {fp:4}\")\n",
    "    print(f\"     1 {fn:4} {tp:4}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"fall_out\": fall_out,\n",
    "        \"miss_rate\": miss_rate,\n",
    "        \"confusion_matrix\": {\n",
    "            \"tn\": int(tn), \"fp\": int(fp), \n",
    "            \"fn\": int(fn), \"tp\": int(tp)\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò LLM Judge Classification Notebook\n",
    "\n",
    "# Cell 1: Install dependencies (uncomment to run in Colab)\n",
    "# !pip install openai anthropic ollama google-generativeai\n",
    "\n",
    "# Cell 2: Imports and setup\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "# from google import generativeai as genai\n",
    "# from openai import OpenAI\n",
    "# from anthropic import Anthropic\n",
    "\n",
    "# Cell 3: Extract reasoning\n",
    "\n",
    "# def extract_reasoning(response: str) -> str:\n",
    "#     # print(response)\n",
    "#     for line in response.splitlines():\n",
    "#         if line.lower().startswith(\"reasoning:\"):\n",
    "#             return line.split(\":\", 1)[1].strip()\n",
    "#     return response.strip()\n",
    "def extract_reasoning(response: str) -> str:\n",
    "    lines = response.splitlines()\n",
    "    capture = False\n",
    "    reasoning_parts = []\n",
    "\n",
    "    for line in lines:\n",
    "        if not capture:\n",
    "            if \"reasoning:\" in line.lower():\n",
    "                # Start capturing from after the colon\n",
    "                reasoning_start = line.lower().find(\"reasoning:\")\n",
    "                reasoning_text = line[reasoning_start + len(\"reasoning:\"):].strip()\n",
    "                reasoning_parts.append(reasoning_text)\n",
    "                capture = True\n",
    "        else:\n",
    "            reasoning_parts.append(line.strip())\n",
    "\n",
    "    return \" \".join(reasoning_parts).strip()\n",
    "\n",
    "# # Cell 4: Model classification functions (return prediction, explanation)\n",
    "\n",
    "# def classify_with_openai(text, prompt_template, model=\"gpt-4\"):\n",
    "#     client = OpenAI(api_key=API_KEYS['openai'])\n",
    "#     prompt = prompt_template.format(text=text.strip())\n",
    "#     try:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=model,\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#                 {\"role\": \"user\", \"content\": prompt}\n",
    "#             ],\n",
    "#             temperature=0.0,\n",
    "#             max_tokens=100\n",
    "#         )\n",
    "#         answer = response.choices[0].message.content.strip()\n",
    "#         return 1 if \"yes\" in answer.lower() else 0, extract_reasoning(answer)\n",
    "#     except Exception as e:\n",
    "#         return 0, f\"OpenAI error: {e}\"\n",
    "\n",
    "\n",
    "# def classify_with_claude(text, prompt_template, model=\"claude-3-sonnet-20240229\"):\n",
    "#     client = Anthropic(api_key=API_KEYS['anthropic'])\n",
    "#     prompt = prompt_template.format(text=text.strip())\n",
    "#     try:\n",
    "#         response = client.messages.create(\n",
    "#             model=model,\n",
    "#             system=\"You are a helpful assistant.\",\n",
    "#             max_tokens=100,\n",
    "#             temperature=0.0,\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#         )\n",
    "#         answer = response.content[0].text.strip()\n",
    "#         return 1 if \"yes\" in answer.lower() else 0, extract_reasoning(answer)\n",
    "#     except Exception as e:\n",
    "#         return 0, f\"Claude error: {e}\"\n",
    "\n",
    "\n",
    "# def classify_with_gemini_flash25(text, prompt_template, model=\"gemini-2.5-flash-preview-0513\"):\n",
    "#     genai.configure(api_key=API_KEYS['google'])\n",
    "#     try:\n",
    "#         gem_model = genai.GenerativeModel(model)\n",
    "#         prompt = prompt_template.format(text=text.strip())\n",
    "#         response = gem_model.generate_content(prompt)\n",
    "#         answer = response.text.strip()\n",
    "#         return 1 if \"yes\" in answer.lower() else 0, extract_reasoning(answer)\n",
    "#     except Exception as e:\n",
    "#         return 0, f\"Gemini error: {e}\"\n",
    "\n",
    "\n",
    "def classify_with_ollama(text, prompt_template, model=\"\"):\n",
    "    prompt = prompt_template.format(text=text.strip())\n",
    "    # print(prompt)\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0.7}\n",
    "        )\n",
    "        answer = response['message']['content'].strip()\n",
    "        # print(answer)\n",
    "        return 1 if \"yes\" in answer.lower() else 0, extract_reasoning(answer)\n",
    "    except Exception as e:\n",
    "        return 0, f\"Ollama error: {e}\"\n",
    "\n",
    "# Cell 5: Model function map\n",
    "MODEL_FUNCTIONS = {\n",
    "    # \"openai_o3\": classify_with_openai,\n",
    "    # \"claude_sonnet_4\": classify_with_claude,\n",
    "    # \"gemini_2.5_flash\": classify_with_gemini_flash25,\n",
    "    # \"llama_3.2\": classify_with_ollama,\n",
    "    \"mistral3.1_24B_q4\" : classify_with_ollama\n",
    "}\n",
    "\n",
    "# Cell 6: Helper functions for judge evaluation\n",
    "\n",
    "def get_model_response(text, model_name, template):\n",
    "    classify_func = MODEL_FUNCTIONS[model_name]\n",
    "    model_id = MODELS[model_name]\n",
    "    return classify_func(text, template, model_id)\n",
    "\n",
    "\n",
    "\n",
    "def judge_final_decision(text, lawyer_reasoning, judge_model, judge_prompt):\n",
    "#     judge_prompt = f\"\"\"\n",
    "# You are an expert evaluating if a student's reflection expresses doubt.\n",
    "\n",
    "# Reflection:\n",
    "# \"{text}\"\n",
    "\n",
    "# Here are the model analyses:\n",
    "# \"\"\"\n",
    "#     for m, v in candidates.items():\n",
    "#         label = \"Doubt\" if v[\"prediction\"] == 1 else \"No Doubt\"\n",
    "#         judge_prompt += f\"\\n‚Ä¢ {m}: {label} - {v['explanation']}\"\n",
    "\n",
    "#     judge_prompt += \"\"\"\n",
    "\n",
    "# Please reply in the format:\n",
    "# Classification: [Yes/No]\n",
    "# Reasoning: [Brief explanation]\n",
    "# \"\"\"\n",
    "    # lawyer_reasoning = \"\"\n",
    "    # for m, v in candidates.items():\n",
    "    #     label = \"Doubt\" if v[\"prediction\"] == 1 else \"No Doubt\"\n",
    "    #     lawyer_reasoning += f\"\\n‚Ä¢ {m}: {label} - {v['explanation']}\"\n",
    "\n",
    "    # print(candidates)\n",
    "    # print(lawyer_reasoning)\n",
    "    # print(\"============\")\n",
    "    \n",
    "    print(judge_prompt)\n",
    "    pred, explanation = MODEL_FUNCTIONS[judge_model](text, judge_prompt, MODELS[judge_model])\n",
    "    return pred, explanation\n",
    "\n",
    "# Cell 7: Main pipeline runner\n",
    "\n",
    "def run_judge_pipeline(df, text_col, model_1, model_2, judge_model, prompt_type, output_file=\"judge_results.jsonl\"):\n",
    "    # prompt_template = PROMPTS[prompt_type]\n",
    "\n",
    "    y_pred = []\n",
    "    y_expl = []\n",
    "    all_results = {}\n",
    "    summary_data = []\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        # for i in range(min(10, len(df))):\n",
    "        for i in range(len(df)):\n",
    "            text = df.iloc[i][text_col]\n",
    "            # candidates = {}\n",
    "            lawyer_reasoning = \"\"\n",
    "            # for model in [model_1, model_2]:\n",
    "            #     pred, expl = get_model_response(text, model, prompt_template)\n",
    "            #     candidates[model] = {\"prediction\": pred, \"explanation\": expl}\n",
    "\n",
    "            prosecutor_prompt =  \"\"\"\n",
    "            You are the first tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection and argue that the student express doubts on the topic and requires further explanation on a topic.\n",
    "\n",
    "            Here is the student's reflection:\n",
    "\n",
    "            <student_reflection>\n",
    "            \"{text}\"\n",
    "            </student_reflection>\n",
    "\n",
    "            Only reply Yes. Provide your reasons.\n",
    "            Answer: Yes\n",
    "            Reasoning: ...\n",
    "            \"\"\"\n",
    "            pred_1, expl_1 = get_model_response(text, model_1, prosecutor_prompt)\n",
    "            # candidates[\"prosecutor_prompt\"] = {\"prediction\": pred_1, \"explanation\": expl_1}\n",
    "            label = \"Doubt\" if pred_1 == 1 else \"No Doubt\"\n",
    "            lawyer_reasoning += f\"\\n‚Ä¢ first tutor: {label} - {expl_1 }\"\n",
    "\n",
    "            # print(pred_1)\n",
    "            # print(expl_1)\n",
    "            defender_prompt = f\"\"\"\n",
    "            You are the second tutor for students learning a topic. Each student is writing a reflection on his/her learning. Your task is to analyze the reflection and argue that the student does not express doubts on the topic or requires further explanation on a topic.\n",
    "\n",
    "            Student Reflection:\n",
    "            \"{text}\"\n",
    "\n",
    "            Prosecutor's Argument:\n",
    "            \"{expl_1}\"\n",
    "\n",
    "            Only reply No. Provide your reasons.\n",
    "            Answer: No\n",
    "            Reasoning: ...\n",
    "            \"\"\"\n",
    "\n",
    "            pred_2, expl_2 = get_model_response(text, model_2, defender_prompt)\n",
    "            # candidates[\"defender_prompt\"] = {\"prediction\": pred_2, \"explanation\": expl_2}\n",
    "            label = \"Doubt\" if pred_2 == 1 else \"No Doubt\"\n",
    "            lawyer_reasoning += f\"\\n\\n‚Ä¢ second tutor: {label} - {expl_2 }\"\n",
    "\n",
    "            # print(lawyer_reasoning)\n",
    "\n",
    "            judge_prompt = f\"\"\"\n",
    "            You are an impartial experienced third tutor evaluating a case where the first and second tutor make opposite claims that a student‚Äôs reflection explicilty expressing doubt.\n",
    "\n",
    "            Student Reflection:\n",
    "            \"{text}\"\n",
    "\n",
    "            Here are the prosecutor and defender agent analyses:\n",
    "            \"{lawyer_reasoning}\"\n",
    "\n",
    "            If you analyse student reflection and both tutors analyses and conclude the student explicitly expressing doubt, output Yes. If not, output No. Provide your reasons.\n",
    "\n",
    "            Only reply Yes or No. Provide your reasons.\n",
    "            Answer: Yes or No\n",
    "            Reasoning: ...\n",
    "            \"\"\"\n",
    "            judge_pred, judge_expl = judge_final_decision(text, lawyer_reasoning, judge_model, judge_prompt)\n",
    "            y_pred.append(judge_pred)\n",
    "            y_expl.append(judge_expl)\n",
    "            result = {\n",
    "                \"sample_index\": i,\n",
    "                \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"reflection\": text,\n",
    "                \"candidates\": lawyer_reasoning,\n",
    "                \"judge\": {\n",
    "                    \"model\": judge_model,\n",
    "                    \"prediction\": judge_pred,\n",
    "                    \"explanation\": judge_expl\n",
    "                }\n",
    "            }\n",
    "\n",
    "            f.write(json.dumps(result) + \"\\n\")\n",
    "            print(f\"‚úÖ {i+1}: Judge={judge_pred} - {judge_expl[:60]}...\")\n",
    "\n",
    "        model_results = {}\n",
    "        label_col = CONFIG['dataset']['label_column']\n",
    "        y_true = df[label_col].astype(int).tolist()\n",
    "        metrics = calculate_metrics(y_true, y_pred, f\"{judge_model} ({prompt_type})\")\n",
    "\n",
    "        # Store results\n",
    "        model_results[prompt_type] = {\n",
    "            \"predictions\": y_pred,\n",
    "            \"explanation\": y_expl,\n",
    "            \"metrics\": metrics,\n",
    "            \"sample_size\": len(y_pred)\n",
    "        }\n",
    "        print(y_expl)        \n",
    "        # Add to summary\n",
    "        summary_data.append({\n",
    "            \"Model\": judge_model+\"_\" + timestamp,\n",
    "            \"Prompt\": prompt_type,\n",
    "            \"Accuracy\": metrics[\"accuracy\"],\n",
    "            \"Precision\": metrics[\"precision\"],\n",
    "            \"Recall\": metrics[\"recall\"],\n",
    "            \"Specificity\": metrics[\"specificity\"],\n",
    "            \"F1\": metrics[\"f1\"],\n",
    "            \"F2\": metrics[\"f2\"],\n",
    "            \"Fall_Out\": metrics[\"fall_out\"],\n",
    "            \"Miss_Rate\": metrics[\"miss_rate\"],\n",
    "            \"Sample_Size\": len(y_pred)\n",
    "            }\n",
    "        \n",
    "        )\n",
    "        # Store model results\n",
    "        all_results[judge_model+\"_\" + timestamp] = model_results\n",
    "\n",
    "        # SAVE RESULTS \n",
    "        print(\"\\nüíæ Saving results for each model...\")\n",
    "\n",
    "        # Prepare results for saving\n",
    "        results_to_save = {\n",
    "            'all_results': all_results,\n",
    "            'summary_data': summary_data,\n",
    "            'testing_config': {\n",
    "                'dataset_size': len(df),\n",
    "                'prompt_strategies': list(PROMPTS.keys()),\n",
    "                'total_combinations_tested': len(summary_data)\n",
    "            },\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "        # Save results\n",
    "        try:\n",
    "            \n",
    "            fsave = \"judge\"\n",
    "            output_dir = \"output\"\n",
    "        \n",
    "            # Save as pickle (preserves Python objects) - both versions\n",
    "            pickle_filename = f\"{output_dir}/{fsave}_{judge_model}_{timestamp}.pkl\"\n",
    "            \n",
    "            with open(pickle_filename, 'wb') as f:\n",
    "                pickle.dump(results_to_save, f)\n",
    "            print(f\"üíæ Results saved to {pickle_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving results: {e}\")\n",
    "# Cell 8: Run example\n",
    "fsave = \"judge\"\n",
    "output_dir = \"output\"\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "model1 = \"mistral3.1_24B_q4\"\n",
    "model2 = \"mistral3.1_24B_q4\"\n",
    "judge = \"mistral3.1_24B_q4\"\n",
    "\n",
    "run_judge_pipeline(df, CONFIG['dataset']['text_column'], model1, model2, judge, \"judge\", f\"{output_dir}/{fsave}_{model1}_{model2}_{judge}_{timestamp}.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
