{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ddba1b",
   "metadata": {},
   "source": [
    "1. Ensure there are PKL files in output/llm and output/slm. You may need to run Step 1-2 of llm_doubts.ipynb to load the packages first\n",
    "2. Run Step 1 to merge the PKL files into output_merged/llm_doubts.pkl and output_merged/slm_doubts.pkl \n",
    "3. To view LLMs results, do Step 2\n",
    "4. To view SLMs results, do Step 3\n",
    "5. To view ensemble results, do Step 4-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Merge LLMs and SLMs\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_pickle(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(obj, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def merge_lists(list1, list2):\n",
    "    existing = set(repr(item) for item in list1)\n",
    "    for item in list2:\n",
    "        if repr(item) not in existing:\n",
    "            list1.append(item)\n",
    "    return list1\n",
    "\n",
    "def merge_all_keys(folder_path, output_file):\n",
    "    merged = {}\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.pkl')]\n",
    "    files.sort()\n",
    "\n",
    "    for fname in files:\n",
    "        path = os.path.join(folder_path, fname)\n",
    "        data = load_pickle(path)\n",
    "        print(f\"‚úÖ Processing: {fname}\")\n",
    "\n",
    "        for key, value in data.items():\n",
    "            if key not in merged:\n",
    "                merged[key] = value\n",
    "            else:\n",
    "                if isinstance(merged[key], list) and isinstance(value, list):\n",
    "                    merged[key] = merge_lists(merged[key], value)\n",
    "                elif isinstance(merged[key], list):\n",
    "                    merged[key].append(value)\n",
    "                elif isinstance(value, list):\n",
    "                    merged[key] = [merged[key]] + value\n",
    "                else:\n",
    "                    merged[key] = [merged[key], value]  # append both values as list\n",
    "\n",
    "    save_pickle(merged, output_file)\n",
    "    print(f\"üíæ Merged result saved to {output_file}\")\n",
    "\n",
    "folder_path = \"output/llm\"\n",
    "merged_file = \"output_merged/llm_doubts.pkl\"\n",
    "merge_all_keys(folder_path, merged_file)\n",
    "\n",
    "# data = load_pickle(merged_file)\n",
    "\n",
    "folder_path = \"output/slm\"\n",
    "merged_file = \"output_merged/slm_doubts.pkl\"\n",
    "merge_all_keys(folder_path, merged_file)\n",
    "\n",
    "folder_path = \"output/mad\"\n",
    "merged_file = \"output_merged/slm_mad_doubts.pkl\"\n",
    "merge_all_keys(folder_path, merged_file)\n",
    "\n",
    "# data = load_pickle(merged_file)\n",
    "# print_values(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - LLMS RESULTS ANALYSIS AND SUMMARY\n",
    "# ================================================================================\n",
    "import pandas as pd\n",
    "\n",
    "merged_file = \"output_merged/llm_doubts.pkl\"\n",
    "data = load_pickle(merged_file)\n",
    "summary_data = data['summary_data']\n",
    "# Create comprehensive summary\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\nüìä LLM INDIVIDUAL MODEL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(summary_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # Find best performers\n",
    "    if len(summary_df) > 0:\n",
    "        best_f1_idx = summary_df['F1'].idxmax()\n",
    "        best_f1_row = summary_df.loc[best_f1_idx]\n",
    "        \n",
    "        best_accuracy_idx = summary_df['Accuracy'].idxmax()\n",
    "        best_accuracy_row = summary_df.loc[best_accuracy_idx]\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST PERFORMERS:\")\n",
    "        print(f\"  ‚Ä¢ Best F1: {best_f1_row['Model']} with {best_f1_row['Prompt']} (F1: {best_f1_row['F1']:.4f})\")\n",
    "        print(f\"  ‚Ä¢ Best Accuracy: {best_accuracy_row['Model']} with {best_accuracy_row['Prompt']} (Acc: {best_accuracy_row['Accuracy']:.4f})\")\n",
    "        \n",
    "        # Performance distribution\n",
    "        print(f\"\\nüìä PERFORMANCE DISTRIBUTION:\")\n",
    "        print(f\"  ‚Ä¢ F1 Score range: {summary_df['F1'].min():.4f} - {summary_df['F1'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ F2 Score range: {summary_df['F2'].min():.4f} - {summary_df['F2'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Specificity range: {summary_df['Specificity'].min():.4f} - {summary_df['Specificity'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Mean F1 Score: {summary_df['F1'].mean():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Mean F2 Score: {summary_df['F2'].mean():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Std F1 Score: {summary_df['F1'].std():.4f}\")\n",
    "        \n",
    "        # Identify best performers by different metrics\n",
    "        print(f\"\\nüéØ BEST PERFORMERS BY METRIC:\")\n",
    "        best_f1 = summary_df.loc[summary_df['F1'].idxmax()]\n",
    "        best_f2 = summary_df.loc[summary_df['F2'].idxmax()]\n",
    "        best_spec = summary_df.loc[summary_df['Specificity'].idxmax()]\n",
    "        best_prec = summary_df.loc[summary_df['Precision'].idxmax()]\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Best F1 Score: {best_f1['Model']} ({best_f1['Prompt']}) = {best_f1['F1']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best F2 Score: {best_f2['Model']} ({best_f2['Prompt']}) = {best_f2['F2']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best Specificity: {best_spec['Model']} ({best_spec['Prompt']}) = {best_spec['Specificity']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best Precision: {best_prec['Model']} ({best_prec['Prompt']}) = {best_prec['Precision']:.4f}\")\n",
    "        \n",
    "        # Sample size info\n",
    "        if len(summary_df['Sample_Size'].unique()) > 1:\n",
    "            print(f\"\\nüìè SAMPLE SIZES:\")\n",
    "            for _, row in summary_df.iterrows():\n",
    "                print(f\"  ‚Ä¢ {row['Model']} ({row['Prompt']}): {row['Sample_Size']} samples\")\n",
    "        else:\n",
    "            print(f\"\\nüìè All tests used {summary_df['Sample_Size'].iloc[0]} samples\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå No results to analyze\")\n",
    "\n",
    "print(f\"\\n‚úÖ Individual model testing analysis complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b991c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - SLMS RESULTS ANALYSIS AND SUMMARY\n",
    "# ================================================================================\n",
    "import pandas as pd\n",
    "\n",
    "merged_file = \"output_merged/slm_doubts.pkl\"\n",
    "data = load_pickle(merged_file)\n",
    "\n",
    "summary_data = data['summary_data']\n",
    "# Create comprehensive summary\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\nüìä SLM INDIVIDUAL MODEL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(summary_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # Find best performers\n",
    "    if len(summary_df) > 0:\n",
    "        best_f1_idx = summary_df['F1'].idxmax()\n",
    "        best_f1_row = summary_df.loc[best_f1_idx]\n",
    "        \n",
    "        best_accuracy_idx = summary_df['Accuracy'].idxmax()\n",
    "        best_accuracy_row = summary_df.loc[best_accuracy_idx]\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST PERFORMERS:\")\n",
    "        print(f\"  ‚Ä¢ Best F1: {best_f1_row['Model']} with {best_f1_row['Prompt']} (F1: {best_f1_row['F1']:.4f})\")\n",
    "        print(f\"  ‚Ä¢ Best Accuracy: {best_accuracy_row['Model']} with {best_accuracy_row['Prompt']} (Acc: {best_accuracy_row['Accuracy']:.4f})\")\n",
    "        \n",
    "        # Performance distribution\n",
    "        print(f\"\\nüìä PERFORMANCE DISTRIBUTION:\")\n",
    "        print(f\"  ‚Ä¢ F1 Score range: {summary_df['F1'].min():.4f} - {summary_df['F1'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ F2 Score range: {summary_df['F2'].min():.4f} - {summary_df['F2'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Specificity range: {summary_df['Specificity'].min():.4f} - {summary_df['Specificity'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Mean F1 Score: {summary_df['F1'].mean():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Mean F2 Score: {summary_df['F2'].mean():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Std F1 Score: {summary_df['F1'].std():.4f}\")\n",
    "        \n",
    "        # Identify best performers by different metrics\n",
    "        print(f\"\\nüéØ BEST PERFORMERS BY METRIC:\")\n",
    "        best_f1 = summary_df.loc[summary_df['F1'].idxmax()]\n",
    "        best_f2 = summary_df.loc[summary_df['F2'].idxmax()]\n",
    "        best_spec = summary_df.loc[summary_df['Specificity'].idxmax()]\n",
    "        best_prec = summary_df.loc[summary_df['Precision'].idxmax()]\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Best F1 Score: {best_f1['Model']} ({best_f1['Prompt']}) = {best_f1['F1']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best F2 Score: {best_f2['Model']} ({best_f2['Prompt']}) = {best_f2['F2']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best Specificity: {best_spec['Model']} ({best_spec['Prompt']}) = {best_spec['Specificity']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best Precision: {best_prec['Model']} ({best_prec['Prompt']}) = {best_prec['Precision']:.4f}\")\n",
    "        \n",
    "        # Sample size info\n",
    "        if len(summary_df['Sample_Size'].unique()) > 1:\n",
    "            print(f\"\\nüìè SAMPLE SIZES:\")\n",
    "            for _, row in summary_df.iterrows():\n",
    "                print(f\"  ‚Ä¢ {row['Model']} ({row['Prompt']}): {row['Sample_Size']} samples\")\n",
    "        else:\n",
    "            print(f\"\\nüìè All tests used {summary_df['Sample_Size'].iloc[0]} samples\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå No results to analyze\")\n",
    "\n",
    "print(f\"\\n‚úÖ Individual model testing analysis complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - SLMS MAD RESULTS ANALYSIS AND SUMMARY\n",
    "# ================================================================================\n",
    "import pandas as pd\n",
    "\n",
    "merged_file = \"output_merged/slm_mad_doubts.pkl\"\n",
    "data = load_pickle(merged_file)\n",
    "\n",
    "summary_data = data['summary_data']\n",
    "# Create comprehensive summary\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\nüìä SLM INDIVIDUAL MODEL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(summary_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # Find best performers\n",
    "    if len(summary_df) > 0:\n",
    "        best_f1_idx = summary_df['F1'].idxmax()\n",
    "        best_f1_row = summary_df.loc[best_f1_idx]\n",
    "        \n",
    "        best_accuracy_idx = summary_df['Accuracy'].idxmax()\n",
    "        best_accuracy_row = summary_df.loc[best_accuracy_idx]\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST PERFORMERS:\")\n",
    "        print(f\"  ‚Ä¢ Best F1: {best_f1_row['Model']} with {best_f1_row['Prompt']} (F1: {best_f1_row['F1']:.4f})\")\n",
    "        print(f\"  ‚Ä¢ Best Accuracy: {best_accuracy_row['Model']} with {best_accuracy_row['Prompt']} (Acc: {best_accuracy_row['Accuracy']:.4f})\")\n",
    "        \n",
    "        # Performance distribution\n",
    "        print(f\"\\nüìä PERFORMANCE DISTRIBUTION:\")\n",
    "        print(f\"  ‚Ä¢ F1 Score range: {summary_df['F1'].min():.4f} - {summary_df['F1'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ F2 Score range: {summary_df['F2'].min():.4f} - {summary_df['F2'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Specificity range: {summary_df['Specificity'].min():.4f} - {summary_df['Specificity'].max():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Mean F1 Score: {summary_df['F1'].mean():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Mean F2 Score: {summary_df['F2'].mean():.4f}\")\n",
    "        print(f\"  ‚Ä¢ Std F1 Score: {summary_df['F1'].std():.4f}\")\n",
    "        \n",
    "        # Identify best performers by different metrics\n",
    "        print(f\"\\nüéØ BEST PERFORMERS BY METRIC:\")\n",
    "        best_f1 = summary_df.loc[summary_df['F1'].idxmax()]\n",
    "        best_f2 = summary_df.loc[summary_df['F2'].idxmax()]\n",
    "        best_spec = summary_df.loc[summary_df['Specificity'].idxmax()]\n",
    "        best_prec = summary_df.loc[summary_df['Precision'].idxmax()]\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Best F1 Score: {best_f1['Model']} ({best_f1['Prompt']}) = {best_f1['F1']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best F2 Score: {best_f2['Model']} ({best_f2['Prompt']}) = {best_f2['F2']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best Specificity: {best_spec['Model']} ({best_spec['Prompt']}) = {best_spec['Specificity']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Best Precision: {best_prec['Model']} ({best_prec['Prompt']}) = {best_prec['Precision']:.4f}\")\n",
    "        \n",
    "        # Sample size info\n",
    "        if len(summary_df['Sample_Size'].unique()) > 1:\n",
    "            print(f\"\\nüìè SAMPLE SIZES:\")\n",
    "            for _, row in summary_df.iterrows():\n",
    "                print(f\"  ‚Ä¢ {row['Model']} ({row['Prompt']}): {row['Sample_Size']} samples\")\n",
    "        else:\n",
    "            print(f\"\\nüìè All tests used {summary_df['Sample_Size'].iloc[0]} samples\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå No results to analyze\")\n",
    "\n",
    "print(f\"\\n‚úÖ Individual model testing analysis complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cff7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Prepare for Ensemble\n",
    "\n",
    "# MAJORITY VOTING ENSEMBLE\n",
    "# ================================================================================\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, fbeta_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, method_name=\"\"):\n",
    "    \"\"\"\n",
    "    Calculate and print classification metrics.\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)  # F2 score (emphasizes recall)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix for additional metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate\n",
    "    fall_out = fp / (fp + tn) if (fp + tn) > 0 else 0     # False Positive Rate (1 - specificity)\n",
    "    miss_rate = fn / (fn + tp) if (fn + tp) > 0 else 0    # False Negative Rate (1 - recall)\n",
    "\n",
    "    print(f\"\\nüìä {method_name} Metrics:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Precision: {precision:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Recall:    {recall:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Specificity: {specificity:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Fall Out:    {fall_out:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Miss Rate:   {miss_rate:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F1 Score:    {f1:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F2 Score:    {f2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2\n",
    "    }\n",
    "\n",
    "def get_predictions_for_ensemble(models_list, prompt_type, all_results):\n",
    "    \"\"\"\n",
    "    Get aligned predictions from multiple models for ensemble.\n",
    "    \"\"\"\n",
    "    predictions_dict = {}\n",
    "    min_length = float('inf')\n",
    "    \n",
    "\n",
    "    # Collect predictions from each model\n",
    "    for model_name in models_list:\n",
    "        for result_dict in all_results:\n",
    "            for model_name1, model_results in result_dict.items():\n",
    "                if model_name==model_name1 and prompt_type in model_results and 'predictions' in model_results[prompt_type]:\n",
    "                    preds = model_results[prompt_type]['predictions']\n",
    "                    predictions_dict[model_name] = preds\n",
    "                    min_length = min(min_length, len(preds))\n",
    "    \n",
    "    # Align predictions to same length\n",
    "    aligned_predictions = {}\n",
    "    \n",
    "    for model_name, preds in predictions_dict.items():\n",
    "        aligned_predictions[model_name] = preds[:min_length]\n",
    "    \n",
    "    return aligned_predictions, min_length\n",
    "\n",
    "def majority_vote_ensemble(models_list, prompt_type, all_results):\n",
    "    \"\"\"\n",
    "    Implement majority voting ensemble.\n",
    "    \"\"\"\n",
    "    predictions_dict, sample_count = get_predictions_for_ensemble(models_list, prompt_type, all_results)\n",
    "    \n",
    "    if len(predictions_dict) < 2:\n",
    "        print(f\"‚ùå Need at least 2 models, got {len(predictions_dict)}\")\n",
    "        return None, None, []\n",
    "    \n",
    "    print(f\"ü§ù Majority Vote with {len(predictions_dict)} models:\")\n",
    "    print(f\"  ‚Ä¢ Models: {list(predictions_dict.keys())}\")\n",
    "    print(f\"  ‚Ä¢ Sample count: {sample_count}\")\n",
    "    \n",
    "    # Perform majority voting\n",
    "    ensemble_predictions = []\n",
    "    agreement_scores = []\n",
    "    \n",
    "    for i in range(sample_count):\n",
    "        votes = [predictions_dict[model][i] for model in predictions_dict.keys()]\n",
    "        majority_vote = 1 if sum(votes) > len(votes) / 2 else 0\n",
    "        ensemble_predictions.append(majority_vote)\n",
    "        \n",
    "        # Calculate agreement (how many models agreed with majority)\n",
    "        agreement = sum(1 for vote in votes if vote == majority_vote) / len(votes)\n",
    "        agreement_scores.append(agreement)\n",
    "\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    load_dotenv()\n",
    "    CONFIG = {\n",
    "        'dataset': {\n",
    "            'csv_path': os.getenv(\"DATASET\"),  # UPDATE THIS PATH\n",
    "            'text_column': os.getenv(\"REFLECTION_COLUMN\", \"REFLECTION\"),  # Default text column\n",
    "            'label_column': os.getenv(\"LABEL_COLUMN\", \"label\")  # Default label column\n",
    "        }\n",
    "    }    \n",
    "    try:\n",
    "        df = pd.read_csv(CONFIG['dataset']['csv_path'])\n",
    "        print(f\"‚úÖ Successfully loaded {len(df)} rows\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: File {csv_path} not found.\")\n",
    "        print(\"Please ensure the CSV file exists and update the csv_path in CONFIG.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return None, None\n",
    "     # Create final labels list\n",
    "    \n",
    "    label_col = CONFIG['dataset']['label_column']\n",
    "    y_true = df[label_col].astype(int).tolist()\n",
    "\n",
    "    # Calculate metrics\n",
    "    y_true_subset = y_true[:sample_count]\n",
    "    metrics = calculate_metrics(y_true_subset, ensemble_predictions, \n",
    "                              f\"Majority Vote ({prompt_type})\")\n",
    "    import numpy as np\n",
    "\n",
    "    # Agreement statistics\n",
    "    mean_agreement = np.mean(agreement_scores)\n",
    "    print(f\"  ‚Ä¢ Average model agreement: {mean_agreement:.3f}\")\n",
    "    print(f\"  ‚Ä¢ High agreement samples (>0.8): {sum(1 for a in agreement_scores if a > 0.8)}\")\n",
    "    \n",
    "\n",
    "    # Printing Curve\n",
    "    # Dictionary of predicted scores from different models\n",
    "    # model_scores = predictions_dict\n",
    "    # model_scores[\"majority_vote\"] = ensemble_predictions\n",
    "\n",
    "    # plt.figure(figsize=(7, 6))\n",
    "\n",
    "    # # Plot ROC for each model\n",
    "    # for model_name, scores in model_scores.items():\n",
    "    #     fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    #     roc_auc = auc(fpr, tpr)\n",
    "    #     # Mapping old names to new names\n",
    "    #     name_mapping = {\n",
    "    #         \"openai_o3\" : \"GPT-4o3\",\n",
    "    #         \"claude_sonnet_4\": \"Claude-4\",\n",
    "    #         \"gemini_2.5_flash\": \"Gemini-2.5\",\n",
    "    #         \"majority_vote\" : \"Majority_Vote\"\n",
    "    #     }\n",
    "    #     title = name_mapping.get(model_name, model_name)\n",
    "\n",
    "    #     plt.plot(fpr, tpr, lw=2, label=f'{title} (AUROC = {roc_auc:.2f})')\n",
    "\n",
    "    # # Diagonal baseline\n",
    "    # plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)\n",
    "\n",
    "    # # Plot AUROC settings\n",
    "    # plt.xlabel('False Positive Rate')\n",
    "    # plt.ylabel('True Positive Rate (Recall)')\n",
    "    # plt.title('ROC Curve Comparison')\n",
    "    # plt.legend(loc='lower right')\n",
    "    # plt.grid(True)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.figure(figsize=(7, 6))\n",
    "\n",
    "    # # Plot PR curve for each model\n",
    "    # for model_name, scores in model_scores.items():\n",
    "    #     precision, recall, _ = precision_recall_curve(y_true, scores)\n",
    "    #     pr_auc = average_precision_score(y_true, scores)\n",
    "    #     plt.plot(recall, precision, lw=2, label=f'{model_name} (PR-AUC = {pr_auc:.2f})')\n",
    "\n",
    "    # # Plot formatting\n",
    "    # plt.xlabel('Recall')\n",
    "    # plt.ylabel('Precision')\n",
    "    # plt.title('Precision-Recall Curve Comparison')\n",
    "    # plt.legend(loc='lower left')\n",
    "    # plt.grid(True)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return ensemble_predictions, metrics, list(predictions_dict.keys())\n",
    "\n",
    "# ANALYZE AVAILABLE DATA FOR ENSEMBLE\n",
    "# ================================================================================\n",
    "\n",
    "def analysis_ensemble_models(all_results): \n",
    "    print(\"\\nüìä ENSEMBLE DATA ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    available_predictions = {}\n",
    "    prediction_counts = {}\n",
    "\n",
    "    for result_dict in all_results:\n",
    "        for model_name, model_results in result_dict.items():\n",
    "            available_predictions[model_name] = {}\n",
    "            for prompt_type, result in model_results.items():\n",
    "                if result and 'predictions' in result:\n",
    "                    available_predictions[model_name][prompt_type] = len(result['predictions'])\n",
    "                    \n",
    "                    # Count for this prompt type\n",
    "                    if prompt_type not in prediction_counts:\n",
    "                        prediction_counts[prompt_type] = 0\n",
    "                    prediction_counts[prompt_type] += 1\n",
    "\n",
    "    # Determine best strategies for ensemble\n",
    "    best_prompt_for_ensemble = max(prediction_counts.keys(), key=lambda k: prediction_counts[k])\n",
    "    models_for_ensemble = [m for m in available_predictions.keys() \n",
    "                        if best_prompt_for_ensemble in available_predictions[m]]\n",
    "\n",
    "    print(f\"\\nüéØ ENSEMBLE STRATEGY:\")\n",
    "    print(f\"  ‚Ä¢ Best prompt type: {best_prompt_for_ensemble} ({prediction_counts[best_prompt_for_ensemble]} models)\")\n",
    "    print(f\"  ‚Ä¢ Models for ensemble: {models_for_ensemble}\")\n",
    "\n",
    "    if len(models_for_ensemble) < 2:\n",
    "        print(f\"\\n‚ö†Ô∏è WARNING: Need at least 2 models for ensemble methods\")\n",
    "        print(f\"   Only {len(models_for_ensemble)} models available\")\n",
    "        ensemble_possible = False\n",
    "    else:\n",
    "        ensemble_possible = True\n",
    "        print(f\"\\n‚úÖ Ensemble methods possible with {len(models_for_ensemble)} models\")\n",
    "\n",
    "    ensemble_predications = {}\n",
    "    # Test majority voting for each prompt type\n",
    "    if ensemble_possible:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MAJORITY VOTING ENSEMBLE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        majority_vote_results = {}\n",
    "        \n",
    "        for prompt_type in [\"zero_shot\", \"one_shot\", \"few_shot\"]:\n",
    "            if prompt_type in prediction_counts and prediction_counts[prompt_type] >= 2:\n",
    "                print(f\"\\nüìù Testing {prompt_type} majority voting...\")\n",
    "                \n",
    "                # Get models available for this prompt type\n",
    "                available_for_prompt = [m for m in available_predictions.keys() \n",
    "                                    if prompt_type in available_predictions[m]]\n",
    "                \n",
    "                predictions, metrics, participating_models = majority_vote_ensemble(\n",
    "                    available_for_prompt, prompt_type, all_results)\n",
    "                \n",
    "                ensemble_predications[prompt_type] = predictions\n",
    "                # print(ensemble_predications)\n",
    "                if predictions and metrics:\n",
    "                    majority_vote_results[prompt_type] = {\n",
    "                        'predictions': predictions,\n",
    "                        'metrics': metrics,\n",
    "                        'participating_models': participating_models,\n",
    "                        'sample_count': len(predictions)\n",
    "                    }\n",
    "            else:\n",
    "                print(f\"\\n‚ùå {prompt_type}: insufficient models ({prediction_counts.get(prompt_type, 0)})\")\n",
    "        \n",
    "        # Show best majority vote result\n",
    "        if majority_vote_results:\n",
    "            best_mv = max(majority_vote_results.items(), key=lambda x: x[1]['metrics']['f1'])\n",
    "            print(f\"\\nüèÜ Best Majority Vote: {best_mv[0]} (F1: {best_mv[1]['metrics']['f1']:.4f})\")\n",
    "            print(f\"üèÜ Best Majority Vote: {best_mv[0]} (F2: {best_mv[1]['metrics']['f2']:.4f})\")\n",
    "            print(f\"   Models used: {best_mv[1]['participating_models']}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Skipping majority voting - insufficient models\")\n",
    "        majority_vote_results = {}\n",
    "\n",
    "    return ensemble_predications\n",
    "\n",
    "\n",
    "# Load individual model results\n",
    "try:\n",
    "    with open('output_merged/llm_doubts.pkl', 'rb') as f:\n",
    "        individual_results_llm = pickle.load(f)\n",
    "    print(\"‚úÖ Individual model results llm loaded\")\n",
    "    all_results_llm = individual_results_llm['all_results']\n",
    "    print(f\"üìä Loaded results for {len(all_results_llm)} models\")\n",
    "\n",
    "    with open('output_merged/slm_doubts.pkl', 'rb') as f:\n",
    "        individual_results_slm = pickle.load(f)\n",
    "    print(\"‚úÖ Individual model results slm loaded\")\n",
    "    all_results_slm = individual_results_slm['all_results']\n",
    "    print(f\"üìä Loaded results for {len(individual_results_slm)} models\")\n",
    "\n",
    "    with open('output_merged/slm_mad_doubts.pkl', 'rb') as f:\n",
    "        individual_results_slm_mad = pickle.load(f)\n",
    "    print(\"‚úÖ Individual model results slm mad loaded\")\n",
    "    all_results_slm_mad = individual_results_slm_mad['all_results']\n",
    "    print(f\"üìä Loaded results for {len(individual_results_slm_mad)} models\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Individual model results not found. Run Script 2 first.\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d18c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predications_llm = analysis_ensemble_models(all_results_llm)\n",
    "print(ensemble_predications_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef3c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predications_slm = analysis_ensemble_models(all_results_slm)\n",
    "print(ensemble_predications_slm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ROC for all models\n",
    "\n",
    "predictions_dict = {}\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "CONFIG = {\n",
    "    'dataset': {\n",
    "        'csv_path': os.getenv(\"DATASET\"),  # UPDATE THIS PATH\n",
    "        'text_column': os.getenv(\"REFLECTION_COLUMN\", \"REFLECTION\"),  # Default text column\n",
    "        'label_column': os.getenv(\"LABEL_COLUMN\", \"label\")  # Default label column\n",
    "    }\n",
    "}   \n",
    "try:\n",
    "    df = pd.read_csv(CONFIG['dataset']['csv_path'])\n",
    "    print(f\"‚úÖ Successfully loaded {len(df)} rows\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: File {csv_path} not found.\")\n",
    "    print(\"Please ensure the CSV file exists and update the csv_path in CONFIG.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading file: {e}\")\n",
    "\n",
    "label_col = CONFIG['dataset']['label_column']\n",
    "y_true = df[label_col].astype(int).tolist()\n",
    "\n",
    "# Collect predictions from each model\n",
    "for result_dict in all_results_llm:\n",
    "    for model_name, model_results in result_dict.items():\n",
    "            for prompt, prediction_results in model_results.items():\n",
    "                print(prompt)\n",
    "                if prompt != \"one_shot\":\n",
    "                    preds = prediction_results['predictions']\n",
    "                    predictions_dict[model_name+\"_\"+prompt] = preds\n",
    "predictions_dict[\"Maj_Vote_LLM(zero-shot)\"] = ensemble_predications_llm[\"zero_shot\"]\n",
    "predictions_dict[\"Maj_Vote_LLM(few-shot)\"] = ensemble_predications_llm[\"few_shot\"]\n",
    "\n",
    "for result_dict in all_results_slm:\n",
    "    for model_name, model_results in result_dict.items():\n",
    "            for prompt, prediction_results in model_results.items():\n",
    "                if prompt != \"one_shot\":\n",
    "                    preds = prediction_results['predictions']\n",
    "                    predictions_dict[model_name+\"_\"+prompt] = preds\n",
    "predictions_dict[\"Maj_Vote_SLM(zero-shot)\"] = ensemble_predications_llm[\"zero_shot\"]\n",
    "predictions_dict[\"Maj_Vote_SLM(few-shot)\"] = ensemble_predications_llm[\"few_shot\"]\n",
    "\n",
    "for result_dict in all_results_slm_mad:\n",
    "    for model_name, model_results in result_dict.items():\n",
    "            for prompt, prediction_results in model_results.items():\n",
    "                if prompt != \"one_shot\":\n",
    "                    preds = prediction_results['predictions']\n",
    "                    predictions_dict[model_name+\"_\"+prompt] = preds\n",
    "\n",
    "\n",
    "# print(predictions_dict)\n",
    "# Printing Curve\n",
    "# Dictionary of predicted scores from different models\n",
    "model_scores = predictions_dict\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "\n",
    "underline = '\\u0332'  # combining underline character\n",
    "\n",
    "def underline_text(text):\n",
    "    return ''.join(c + underline for c in text)\n",
    "\n",
    "# Plot ROC for each model\n",
    "for model_name, scores in model_scores.items():\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    # print(y_true)\n",
    "    # print(scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    name_mapping = {\n",
    "        \"openai_o3_zero_shot\" : \"GPT(zero-shot)\",\n",
    "        \"openai_o3_few_shot\" : \"GPT(few-shot)\",\n",
    "        \"claude_sonnet_4_zero_shot\": \"Claude(zero-shot)\",\n",
    "        \"claude_sonnet_4_few_shot\": \"Claude(few-shot)\",\n",
    "        \"gemini_2.5_flash_zero_shot\": \"Gemini(zero-shot)\",\n",
    "        \"gemini_2.5_flash_few_shot\": \"Gemini(few-shot)\",\n",
    "        \"llama_3.2_20250610_123907_zero_shot\": \"Llama(zero-shot)\",\n",
    "        \"llama_3.2_20250610_123907_few_shot\": \"Llama(few-shot)\",\n",
    "        \"mistral3.1_24B_zero_shot\": \"Mistral(zero-shot)\",\n",
    "        \"mistral3.1_24B_few_shot\": \"Mistral(few-shot)\",\n",
    "        \"deepseek_r1_zero_shot\": \"Deepseek(zero-shot)\",\n",
    "        \"deepseek_r1_few_shot\": \"Deepseek(few-shot)\",\n",
    "        \"qwen3_8b_q8_zero_shot\": \"Qwen(zero-shot)\",\n",
    "        \"qwen3_8b_q8_few_shot\": \"Qwen(few-shot)\",\n",
    "        \"mistral3.1_24B_q4_20250618_202359_judge\": \"SLM-as-a-Judge\",\n",
    "        \"mistral3.1_24B_q4_20250613_234232_self_consistency\": \"Self-Consistency\",\n",
    "        \"mistral3.1_24B_q4_20250618_134538_two_agents_chain\": \"Two-Agents-Chain\"\n",
    "    }\n",
    "    title = name_mapping.get(model_name, model_name)\n",
    "    if roc_auc > 0.8:\n",
    "        label = \"AUROC=\"+underline_text(f'({roc_auc:.2f}) {title} ')\n",
    "    else:\n",
    "        label = f'(AUROC={roc_auc:.2f}) {title} '\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=2, label=label)\n",
    "    \n",
    "\n",
    "# Diagonal baseline\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)\n",
    "\n",
    "# Plot AUROC settings\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # Sort legend entries by AUROC descending\n",
    "# handles, labels = plt.gca().get_legend_handles_labels()\n",
    "# sorted_pairs = sorted(zip(labels, handles), reverse=True)\n",
    "# labels, handles = zip(*sorted_pairs)\n",
    "# plt.legend(handles, labels, loc='lower right', fontsize=8)\n",
    "# plt.figure(figsize=(7, 6))\n",
    "\n",
    "# # Plot PR curve for each model\n",
    "for model_name, scores in model_scores.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_true, scores)\n",
    "    pr_auc = average_precision_score(y_true, scores)\n",
    "    plt.plot(recall, precision, lw=2, label=f'{model_name} (PR-AUC = {pr_auc:.2f})')\n",
    "\n",
    "# # Plot formatting\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision-Recall Curve Comparison')\n",
    "# plt.legend(loc='lower left')\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# # Example structure: list of (label, fpr, tpr) tuples\n",
    "# roc_data = model_scores\n",
    "\n",
    "# # Compute AUROC for each\n",
    "# auroc_list = []\n",
    "# for label, fpr, tpr in roc_data:\n",
    "#     score = auc(fpr, tpr)\n",
    "#     auroc_list.append((score, label, fpr, tpr))\n",
    "\n",
    "# # Sort by AUROC descending\n",
    "# auroc_list.sort(reverse=True)\n",
    "\n",
    "# # Plot curves\n",
    "# for score, label, fpr, tpr in auroc_list:\n",
    "#     plt.plot(fpr, tpr, label=f\"{label} (AUROC = {score:.2f})\")\n",
    "\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "# plt.title(\"ROC Curve Comparison\")\n",
    "# plt.grid(True, linestyle='--', alpha=0.5)\n",
    "# plt.legend(loc=\"lower right\", fontsize=8)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Replace these with your actual lists\n",
    "true_labels = y_true             # Your true label list (0 or 1)\n",
    "predicted_labels = model_scores.get('mistral3.1_24B_q4_20250613_234232_self_consistency')       # Your predicted label list (0 or 1)\n",
    "text_col = CONFIG['dataset']['text_column']\n",
    "texts = df[text_col].tolist()                 # The corresponding reflections\n",
    "# print(texts)\n",
    "\n",
    "# Step 1: Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': pd.Series(texts),\n",
    "    'true_label': pd.Series(true_labels),\n",
    "    'predicted_label': pd.Series(predicted_labels)\n",
    "})\n",
    "\n",
    "# Step 2: Identify false positives and false negatives\n",
    "false_positives = df[(df['true_label'] == 0) & (df['predicted_label'] == 1)]\n",
    "false_negatives = df[(df['true_label'] == 1) & (df['predicted_label'] == 0)]\n",
    "\n",
    "# print(len(false_positives))\n",
    "# print(len(false_negatives))\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "print(false_negatives['text'])\n",
    "# Filter for non-empty texts (and optionally strip whitespace)\n",
    "# non_empty_texts = [t for t in texts if t.strip()]\n",
    "\n",
    "# # Check before vectorizing\n",
    "# if len(non_empty_texts) > 0:\n",
    "#     vectorizer = CountVectorizer(stop_words='english')\n",
    "#     X = vectorizer.fit_transform(non_empty_texts)\n",
    "#     # Now continue with analysis\n",
    "# else:\n",
    "#     print(\"No valid texts to analyze.\")\n",
    "\n",
    "# print(len(y_true))\n",
    "# # print(model_scores)\n",
    "# print(len(model_scores.get('claude_sonnet_4_zero_shot')))\n",
    "# print(len(texts))\n",
    "\n",
    "# Step 3: Count most common n-grams (unigrams + bigrams) in each group\n",
    "def get_top_phrases(text_series, n=10):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "    X = vectorizer.fit_transform(text_series)\n",
    "    sum_words = X.sum(axis=0).A1\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    freq = list(zip(vocab, sum_words))\n",
    "    return sorted(freq, key=lambda x: -x[1])[:n]\n",
    "\n",
    "top_fp_phrases = get_top_phrases(false_positives['text'])\n",
    "top_fn_phrases = get_top_phrases(false_negatives['text'])\n",
    "\n",
    "# Print results\n",
    "print(\"üîç Top phrases in False Positives:\")\n",
    "for phrase, count in top_fp_phrases:\n",
    "    print(f\"{phrase}: {count}\")\n",
    "\n",
    "print(\"\\nüîç Top phrases in False Negatives:\")\n",
    "for phrase, count in top_fn_phrases:\n",
    "    print(f\"{phrase}: {count}\")\n",
    "\n",
    "# print()\n",
    "\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "# texts = [...]  # List of reflection texts\n",
    "# true_labels = [...]  # List of true labels\n",
    "# model_predictions = {\n",
    "#     \"gpt4\": [...],\n",
    "#     \"claude\": [...],\n",
    "#     \"gemini\": [...],\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "# Step 1: Track misclassified indices\n",
    "fp_counts = defaultdict(int)\n",
    "fn_counts = defaultdict(int)\n",
    "\n",
    "for model_name, preds in model_scores.items():\n",
    "    for i, (true, pred) in enumerate(zip(true_labels, preds)):\n",
    "        if pred == 1 and true == 0:\n",
    "            fp_counts[i] += 1  # False Positive\n",
    "        elif pred == 0 and true == 1:\n",
    "            fn_counts[i] += 1  # False Negative\n",
    "\n",
    "# Step 2: Sort by most frequent misclassification\n",
    "most_common_fp = sorted(fp_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "most_common_fn = sorted(fn_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 3: Display most common text examples\n",
    "print(\"\\nüî¥ Most Common False Positives:\")\n",
    "for idx, count in most_common_fp[:100]:\n",
    "    print(f\"({count} models) - {texts[idx]}\")\n",
    "\n",
    "print(\"\\nüîµ Most Common False Negatives:\")\n",
    "for idx, count in most_common_fn[:100]:\n",
    "    print(f\"({count} models) - {texts[idx]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
